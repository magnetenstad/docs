<!DOCTYPE html>
<html>
  <head>
    <!-- Katex -->
    <link rel="stylesheet" href=
        "https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"/>

    <!-- GitHub Markdown Styles -->
    <link rel="stylesheet" href=
        "https://cdn.jsdelivr.net/github-markdown-css/2.2.1/github-markdown.css"/>

    <title>index.md</title>
    <link rel="icon" type="image/x-icon" href="../../favicon.png"/>

    <!-- Custom Styles -->
    <link rel="stylesheet" href="../../styles.css">
  
  </head>

  <body class="markdown-body">
    <div class="page flex-row">
      <div class="col links">
        
<p><h4><a href="../index.html">courses/</a><a href="./index.html">tdt4186 âœ¨</a>
</h4></p>
<ul>
<li>ðŸ“‚ <a href="./lectures/index.html">lectures</a></li>
</ul>
<p><h4>Table of Contents</h4></p>
<nav class="table-of-contents"><ol><li><a href="#tdt4186---operating-systems">TDT4186 - Operating Systems</a><ol><li><a href="#os-development">OS Development</a></li><li><a href="#processes">Processes</a><ol><li><a href="#threads-and-fibers">Threads and Fibers</a></li><li><a href="#synchronization">Synchronization</a></li><li><a href="#deadlocks">Deadlocks</a></li><li><a href="#inter-process-communication-(ipc)">Inter-process communication (IPC)</a></li><li><a href="#uniprocessor-scheduling">Uniprocessor scheduling</a></li></ol></li><li><a href="#memory-management">Memory management</a></li><li><a href="#input-and-output">Input and Output</a></li><li><a href="#file-systems">File Systems</a></li><li><a href="#operating-system-architectures">Operating system architectures</a><ol><li><a href="#cloud-operating-systems">Cloud operating systems</a></li></ol></li><li><a href="#security">Security</a></li></ol></li></ol></nav>
      </div>
      <article class="col content">
        
<h1 id="tdt4186---operating-systems" tabindex="-1">TDT4186 - Operating Systems</h1>
<p>Operating systems know the hardware in detail and provides suitable abstractions, to serve the users and their application programs. Lectures can be found <a href="lectures/index.html">here</a></p>
<h2 id="os-development" tabindex="-1">OS Development</h2>
<p><a href="lectures/01.html">Link to lecture 1</a></p>
<p>The first system softwares were reusable probram libraries: linkers, loaders, debuggers and device drivers. <strong>The first operating systems</strong> were "resident monitors"; they handled interpretation of job control commands, loading and execution of programs and device control.</p>
<p>With multiprogramming, the CPU works on multiple jobs at the same time. In consequence, <strong>the OS must handle concurrent I/O activites, main memory for multiple programs, programs in execution (processes), processor scheduling and multi user operation (security and accounting)</strong>.</p>
<h2 id="processes" tabindex="-1">Processes</h2>
<p><a href="lectures/04.html">Link to lecture 4</a></p>
<p><strong>A process is a program in execution.</strong> They often consist of alternating sequences of "CPU bursts" and "I/O bursts". The <strong>process context</strong> consists of</p>
<ul>
<li>Memory: code, data, and stack segment (text, data, bss, stack, heap)</li>
<li>Contents on processor registers (Instruction pointer, Stack pointer, General purpose registers)</li>
<li>Process state (RUNNING, READY, BLOCKED, etc.)</li>
<li>User ID (and group ID)</li>
<li>Access permissions</li>
<li>Currently used resources (Files, I/O devices, etc.)</li>
</ul>
<p><strong>Scheduling</strong> enables the coordination of concurrent processes. Scheduling algorithms can be user oriented (short reaction times)or system oriented (optimal CPU utilization). <strong>Inter-process communication (IPC)</strong> enables the collaboration of multiple processes. Examples are shared memory and message passing.</p>
<p>From the point of view of the application, calling an operating system service looks like a regular function call, e.g.: <code><span class="hljs-attribute">pid</span> <span class="hljs-operator">=</span> fork()<span class="hljs-comment">;</span></code> (the C library (libc) provides stubs (adapter functions) that call the actual syscall). However, arbitrarily calling code inside the OS kernel is dangerous. Many CPUs provide several <strong>execution modes</strong>: "user mode": only restricted functionality is allowed, and "kernel" or "supervisor mode": full access to all hardware resources.</p>
<table>
<thead>
<tr>
<th>Syscall</th>
<th>Description</th>
<th>Manual section</th>
</tr>
</thead>
<tbody>
<tr>
<td>getpid</td>
<td>returns PID of the calling process</td>
<td>(2)</td>
</tr>
<tr>
<td>getppid</td>
<td>returns PID of the parent process</td>
<td>(2)</td>
</tr>
<tr>
<td>getuid</td>
<td>return the UID of the calling process</td>
<td>(2)</td>
</tr>
<tr>
<td>fork</td>
<td>creates a new child process</td>
<td>(2)</td>
</tr>
<tr>
<td>exit</td>
<td>terminates the calling process</td>
<td>(3)</td>
</tr>
<tr>
<td>_exit</td>
<td>terminates the calling process</td>
<td>(2)</td>
</tr>
<tr>
<td>wait</td>
<td>waits for the termination of a child process</td>
<td>(2)</td>
</tr>
<tr>
<td>execve</td>
<td>loads and starts a program in the context of the calling process</td>
<td>(2)</td>
</tr>
</tbody>
</table>
<p>Read Unix manual pages with <code>man <span class="hljs-tag">&lt;<span class="hljs-name">num</span>&gt;</span> <span class="hljs-tag">&lt;<span class="hljs-name">command</span>&gt;</span></code></p>
<p>With <code><span class="hljs-function"><span class="hljs-type">pid_t</span> <span class="hljs-title">fork</span><span class="hljs-params">(<span class="hljs-type">void</span>)</span></span></code>, the child's PID is returned to the parent process, and 0 is returned to the child process. The child process continues the program from the current line and inherits most parent process attributes (copy on write), but not the Process ID (PID), and parent process ID (PPID). Copy on write is a an optimization which only really copies the memory if it is updated (written to).</p>
<p><code><span class="hljs-keyword">void</span> <span class="hljs-title function_">_exit</span><span class="hljs-params">(<span class="hljs-type">int</span>)</span></code> terminates the calling process and passes an integer argument as "exit status" to the parent process. It also releases the resources allocated by the process. In C, the library function <code><span class="hljs-keyword">exit</span>()</code> should be used, shich additionally releases resources used by the C library.</p>
<p><code><span class="hljs-function"><span class="hljs-type">pid_t</span> <span class="hljs-title">wait</span><span class="hljs-params">(<span class="hljs-type">int</span> *)</span></span></code> blocks the calling process until one of its child processes terminates, or returns immediately if all child processes are already terminated. The return value is the terminated child's PID. Using the <code><span class="hljs-built_in">int</span> *</code> parameter, the caller is passed the child's "exit status".</p>
<p>A terminated process is called a "zombie" until its exit status is requested using wait. The resources allocated to such processes can be released, but the OS project management still needs to know about them (i.e. exit status has to be saved).</p>
<p>If a parent process terminates before a child, the child process is orphaned. The init process (PID 1) adopts all orphaned processes.</p>
<p><code><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">execve</span><span class="hljs-params">(<span class="hljs-type">const</span> <span class="hljs-type">char</span> *command, <span class="hljs-type">const</span> <span class="hljs-type">char</span> *args[], <span class="hljs-type">const</span> <span class="hljs-type">char</span> *encp[])</span></span></code> loads and starts the command passed in, remaining in the same process. Alternatives: <code><span class="hljs-attribute">execl</span></code>, <code><span class="hljs-attribute">execv</span></code>, <code><span class="hljs-attribute">execlp</span></code>, <code><span class="hljs-attribute">execvp</span></code>.</p>
<p>A more complete unix process state diagram:
<img src="lectures/assets/2022-05-18-11-41-31.png" alt=""></p>
<p>Traditional Unix process creation using fork is too heavyweight for some applications.</p>
<h3 id="threads-and-fibers" tabindex="-1">Threads and Fibers</h3>
<p><a href="lectures/05.html">Link to lecture 5</a></p>
<p><strong>Threads</strong> are lightweight (usually kernel-level) processes and can share address space (code + data + bss + heap). Advantages: Faster context switching, complex operations can run in parallel of user I/O. Disadvantages: error-prone, shared access requires coordination, scheduling overhead.</p>
<p><strong>Fibers</strong> are also called user-level threads (or green threads, or featherweight processes). Implemented on application layer, unknown to the OS. Advantages: 1) Extremely fast context switch: only exchange processor registers. 2) No switch to kernel mode required to switch to different fiber 3) Every application can choose the fiber library best suited for it Disadvantages: 1) Blocking a single fiber leads to blocking the whole process (since the OS doesn't know about fibers). 2) No speed advantage from multiprocessor systems.</p>
<table>
<thead>
<tr>
<th></th>
<th>Processes</th>
<th>Threads</th>
<th>Fibers</th>
</tr>
</thead>
<tbody>
<tr>
<td>Address space</td>
<td>separate</td>
<td>common</td>
<td>common</td>
</tr>
<tr>
<td>Kernel visibility</td>
<td>yes</td>
<td>yes</td>
<td>no</td>
</tr>
<tr>
<td>Scheduling</td>
<td>kernel level</td>
<td>kernel level</td>
<td>user space</td>
</tr>
<tr>
<td>Stack</td>
<td>separate per process</td>
<td>separate per thread</td>
<td>can be common</td>
</tr>
<tr>
<td>Switching overhead</td>
<td>very high</td>
<td>high</td>
<td>low</td>
</tr>
</tbody>
</table>
<p>Windows processes provide environment and address space for threads. Every thread has its own stack and CPU register set. User level threads (fibers) are possible, but unusual. Strategy: keep the number of threads low; Use overlapping (asynchronous) I/O.</p>
<p>Linux implements POSIX threads using the pthreads library. All threads and processes are internally managed as tasks, which the scheduler does not differentiate between.</p>
<h3 id="synchronization" tabindex="-1">Synchronization</h3>
<p><a href="lectures/06.html">Link to lecture 6</a></p>
<p>A <strong>race condition</strong> is a situation in which multiple processes access shared data concurrently and at least one of the processes manipulates the data. To avoid race conditions, concurrent processes need to be synchronized (coordinated).</p>
<p>Only a single process can be in a <strong>critical section</strong> at the same time. Solved by mutual exclusion, using the mutex (<a href="https://en.wikipedia.org/wiki/Lock_(computer_science)">lock</a>) abstraction. However, deadlocks must be considered.</p>
<p><strong>The bakery algorithm</strong> is a working solution for the problem of critical sections. Another idea to ensure that a process stays in the critical section is to <strong>suppress interrupts</strong> (because they cause context switches). Many CPUs support <strong>indivisible (atomic) read/modify/write</strong> cycles that can be used to implement lock algorithms.</p>
<p><strong>Semaphores</strong> are an operating system abstraction to exchange synchronization signals between concurrent processes. A semaphore is defined as a non-negative integer with two atomic operations: <code><span class="hljs-built_in">wait</span></code> (decrement) and <code><span class="hljs-keyword">signal</span><span class="hljs-string"></span></code> (increment).</p>
<p>A <strong>monitor</strong> is an abstract data type with implicit synchronization properties. They are for example implemented in Java.</p>
<p>An <strong>actively waiting</strong> process 1) is unable to change the condition it is waiting for on its own 2) unnecessarily impedes other processes which would be able to use the CPU for "useful" work. 3) harms itself: the longer a process holds the processor, the longer it has to wait for other processes to fulfill the condition it is waiting for.</p>
<p>In the case of <strong>passive waiting</strong>, the process is entered into a waiting queue and is not unblocked until the event occurs.</p>
<h3 id="deadlocks" tabindex="-1">Deadlocks</h3>
<p><a href="lectures/07.html">Link to lecture 7</a></p>
<p>A <strong>deadlock</strong> is a situation in which two or more processes are unable to process because each is waiting for one of the others to do something. A deadlock involves passive waiting, with a BLOCKED process state. The <strong>livelock</strong> alternative involves active waiting and an arbitrary process state. Deadlocks are the "lesser evil".</p>
<p>Necessary conditions for a deadlock:</p>
<ol>
<li>Exclusive allocation of resources ("mutual exclusion")</li>
<li>Allocation of additional resources ("hold and wait")</li>
<li>No removing of resources ("no preemption")</li>
<li>A closed chain of processes exists, such that each process holds at least one resource needed by the next process in the chain ("circular wait")</li>
</ol>
<p><strong>Resources</strong> are administered by the operating system and provided to the processes. <strong>Resource allocation graphs</strong> are used to visualize and also automatically detect deadlock situations. They describe the current system state; The nodes are processes and resources, the edges show an allocation or a request.</p>
<p><strong>Reusable resources</strong> are allocated by processes for a certain time and released again afterwards (CPU, main and mass storage, I/O devices, system data structures such as files, process table entries, etc.). A deadlock occurs if two processes each have allocated a reusable resource which is afterwards additionally requested by the respective other process. Access is typically synchronized with mutual exclusion.</p>
<p><strong>Consumable resources</strong> are generated (produced) and destroyed (consumed) while the system is running (Interrupt requests, signals, messages, data from input devices, etc.). A deadlock occurs if two processes each wait for a consumable resource which is produced by the respective other process. Access is typically synchronized with one-sided synchronization.</p>
<p><strong>Indirect methods for preventing deadlocks</strong> are 1) use non blocking approaches 2) only allow atomic resource allocations 3) enable the preemption of resources using virtualization. <strong>Direct methods</strong> prevent circular waiting with continuous requirements analysis and avoidance of "unsafe states".</p>
<p><strong>Banker's algorithm</strong> is a deadlock avoidance algorithm which finds a process sequence that guarantees that the system does not run out of resources even when all processes completely use their "credit limit".</p>
<p>Deadlocks can be accepted ("ostrich algorithm") or detected by creating a waiting graph and search for cycles (O(n)). In the recovery phase, deadlocked processes are terminated and resources are preempted. Methods to avoid/detect deadlocks are very difficult to implement, require too much overhead and are thus not useable. Prevention methods more commonly used and relevant in practice. The risk of deadlock can also be solved by virtualizing resources.</p>
<h3 id="inter-process-communication-(ipc)" tabindex="-1">Inter-process communication (IPC)</h3>
<p><a href="lectures/11.html">Link to lecture 11</a></p>
<p><strong>Inter-Process Communication (IPC)</strong> can involve 1) multiple processes cooperate on a task 2) simultaneous use of information by multiple processes 3) reduction of processing time due to parallelization 4) hiding of processing times due to "background execution".</p>
<p>Processes can communicate by exchanging messages, or by using shared memory (exchange of data by concurrent writes into and reads out of a common memory area).</p>
<p><strong>Message-oriented communication</strong> can be synchronous (Receiver blocks until the message has arrived, Sender blocks until the reception of the message is confirmed) or asynchronous (Sender hands the message to the OS and continues running. Requires buffering.).</p>
<p>Message addressing can be <strong>direct</strong> (using process ID, or port/socket) or <strong>indirect</strong> (using channels (pipes), mailboxes, message queues). <strong>Group addressing</strong> is another dimension, a message is either unicast (sent to exactly one recipient), multicast (sent to a selection of possible recipients), or broadcast (sent to all).</p>
<p>Unix signals are interrupts implemented in software, and are a minimal form of IPC as only the signal number is transmitted. Examples: <code><span class="hljs-attribute">SIGINT</span></code>: Terminate the process (ctrl-C), <code><span class="hljs-attribute">SIGSTOP</span></code>: Suspend process (Ctrl-Z), <code><span class="hljs-attribute">SIGWINCH</span></code>: Window size has changes, <code><span class="hljs-attribute">SIGCHLD</span></code>: Child process terminated, <code><span class="hljs-attribute">SIGSEGV</span></code>: Memory protection violation, <code><span class="hljs-attribute">SIGKILL</span></code>: Process is killed.</p>
<p><strong>Unix shells</strong> (a "shell" around the operating system "core") are text based user interface to start commands (Unix programs). Every executed command is a separate child process.</p>
<p><strong>Standard I/O channels</strong> (stdin, stdout, stderr) are usually connected to the terminal in which the shell runs that started the process. The numerical file descriptors assigned to these channels are 0 to stdin, 1 to stdout and 2 to stderr. <code><span class="hljs-meta prompt_">&gt;</span></code> redirects standard output, <code>&lt;</code> redirects standard input and <code><span class="hljs-string">|</span></code> (pipe) symbol tells the shell to connect the standard output of the left process to the standard input of the right process.</p>
<p><strong>Unix pipes</strong> are channel between to communicating processes with the following properties: unidirectional, buffers (fixed buffer size), reliable transport, stream-oriented.</p>
<p>Doug Mcllroy, the inventor of Unix pipes, described the <a href="https://en.wikipedia.org/wiki/Unix_philosophy">Unix philosophy</a>. It can be summarized as "Do one thing, do it well."</p>
<p><strong>Sockets</strong> are general (bidirectional and buffered) communication endpoints in a computer network. They are described by a domain (protocol family), a type and a protocol. Unix domain sockets work like bidirectional pipes and can be created as special file in the file system. Internet domain sockets are used for inter-computer communication using Internet protocols.</p>
<p><strong>Remote procedure calls (RPC)</strong> work like a function call between different processes. A request message includes request to execute the remote function and the related parameters. A response message includes the result(s) of the remote call.</p>
<h3 id="uniprocessor-scheduling" tabindex="-1">Uniprocessor scheduling</h3>
<p><a href="lectures/12.html">Link to lecture 12</a></p>
<p>Depending on the scheduling level, every process is assigned a logical state representing its dispatch state at a given point in time: 1) short-term scheduling (Âµs-ms) (ready, running, blocked), 2) medium-term scheduling (ms-min) (swapped and ready, swapped and blocked), 3) long-term scheduling (min â€“ hours) (created, terminated).</p>
<p>A <strong>dispatcher</strong> performs various tasks, including context switching, setting up user registers and memory mapping. These are necessary for the process to execute and transfer CPU control to that process. When dispatching, the process changes from the ready state to the running state. The scheduler selects the process, the dispatcher takes the selected process to running state.</p>
<p>In <strong>preemptive scheduling</strong>, a process can be forced to yield (release) the CPU. In non-preemptive scheduling, all processes run to completion.</p>
<table>
<thead>
<tr>
<th>Scheduling approach</th>
<th>Description</th>
<th>Preemptive</th>
</tr>
</thead>
<tbody>
<tr>
<td>First-Come First-Served (FCFS)</td>
<td>Queueing criterion is the arrival time of a process</td>
<td>No</td>
</tr>
<tr>
<td>Round Robin (RR)</td>
<td>The available processor time is split into time slices</td>
<td>Yes</td>
</tr>
<tr>
<td>Virtual Round Robin (VRR)</td>
<td>Processes can use the remaining run time they did not use in their previous time slice</td>
<td>Yes</td>
</tr>
<tr>
<td>Shortest process next (SPN)</td>
<td>Requires knowledge about the process run times</td>
<td>No</td>
</tr>
<tr>
<td>Shortest Remaining Time First (SRTF)</td>
<td>Extends SPN with preemption</td>
<td>Yes</td>
</tr>
<tr>
<td>Highest Response Ratio Next â€“ HRRN</td>
<td>Extends SRTF, considers the aging of processes</td>
<td>Yes</td>
</tr>
<tr>
<td>Feedback (FB)</td>
<td>Short processes obtain an advantage without having to estimate the relative lengths of processes</td>
<td>Yes</td>
</tr>
</tbody>
</table>
<p><strong>Multi-level scheduling</strong> combined multiple scheduling strategies.</p>
<p>When <strong>evaluating scheduling algorithms</strong>, a number of objectives may be optimized for. <strong>User oriented</strong>: 1) Run time (time between start and termination of a process including the waiting time(s)), 2) Response time (time between user input and program response), 3) Tardiness (deadlines, real-time systems), 4) Predictability (processes are always processed identically independent of the load). <strong>System oriented</strong>: 1) Throughput (finish as many processes as possible per time unit), 2) CPU load (keep the CPU busy at all times), 3) Avoid overhead (scheduling decisions, context switches) 4) Fairness (no process should be disadavantaged (e.g. by starvation)) 5) Load balancing (I/O devices should also be utilized uniformly).</p>
<h2 id="memory-management" tabindex="-1">Memory management</h2>
<p><a href="lectures/09.html">Link to lecture 9</a></p>
<p>Access to I/O devices is often slow ("memory gap", "I/O bottleneck"). <strong>Polling</strong> sends a command and then waits until the device returns data. With <strong>interrupts</strong>, the device notifies the program when data is ready.</p>
<p>Caches work efficiently due to two locality principles:</p>
<ul>
<li><strong>Temporal locality</strong>: a program accessing some part of memory is likely to access the same memory soon thereafter</li>
<li><strong>Spatial locality</strong>: a program accessing some part of memory is likely to access nearby memory next</li>
</ul>
<p>The <strong>MMU</strong> translates allowed ("virtual", "logical") addresses to "physical" addresses in main memory using a translation table. Enables security by separating process address spaces. The MMU uses a special cache on the CPU - the translation lookaside buffer (TLB).</p>
<p>With <strong>static memory allocation</strong>, fixed memory areas are allocated to the OS and user processes. Disadvantages: 1) Limited degree of multiprogramming. 2) Limitation of other resources e.g. I/O bandwidth due to buffers that are too small. 3) Unused OS memory cannot be used by application processes (and vice versa). Dynamic memory allocation solves the problems of static memory allocation, but requires placement and replacement strategies.</p>
<p>Free segments of main memory have to be represented. <strong>The bit list approach</strong> is simple, but have some disadvantages: 1) Bit lists can require lots of memory 2) When releasing memory, the size of the memory block to be released has to be known or provided 3) Linear search is slow. <strong>The linked list approach</strong> uses less (but dynamic!) memory and is faster. But it requires a minimum gap size, to store the length and the pointer to the next free gap.</p>
<p><strong>Placement strategies</strong> decide where to allocate memory. Some trivial placement strategies are First fit, Rotating First Fit / Next Fit, Best Fit, Worst Fit (best and worse are sorted after gap size). <strong>The buddy allocation method</strong> splits memory dynamically into areas of a size 2^n.</p>
<p><strong>External fragmentation</strong> involves memory fragments outside of the allocated memory areas which cannot be used. This is a problem with all list based strategies, e.g. first fit, best fit. <strong>Internal fragmentation</strong> involves unused memory inside of allocated memory areas. This is a problem e.g. with the buddy allocator.</p>
<p>The Linux kernel uses buddy allocation. Inside processes, heap memory enables dynamic allocation of memory with the <code><span class="hljs-attribute">malloc</span></code> and <code><span class="hljs-attribute">free</span></code> functions. Secondary storage sections are ofte managed using bitmaps.</p>
<p>With <strong>multiprogramming</strong>, segments of a process may be swapped out to background memory and released in main memory, if the space is needed by another process. Swapping is slow.</p>
<p>With <strong>segmentation</strong>, a process is divided into variable-length segments. Segment details (Base, Limit (length)) are stored in the segmentation tagle. Segmentation cause fragmentation problems. Compaction reduces fragmentation, but is an operation with large overhead.</p>
<p>With <strong>paging</strong>, a process is divided into fixed-length pages, solving the external fragmentation problem. However, page-based addressing creates internal fragmentation (the last page is often not used completely). Small pages reduce internal fragmentation, but increase the size of the page table (and vice versa). The page table maps logical base addresses (location) to physical base addresses (content).</p>
<p>With <strong>multi-level paging</strong>, the table must not contain the whole address space. Subtables can be created at access time (on demand), which saves memory.</p>
<p>The <strong>translation lookaside buffer (TLB)</strong> provides fast access to page address mapping. TLB has to be flushed when the OS switches context. If a page not contained in the TLB is accessed, the related access information is entered into the TLB and an old TLB entry has to be discarded.</p>
<p>For large logical address spaces (e.g. 64 bit addresses), <strong>inverted page tables</strong> may be used. In an inverted page table, the logical address (along with process PID) is the <em>content</em> of the table, and the physical base address is represented by the location.</p>
<p>With <strong>virtual memory</strong>, the illusion of a large (or "endless") main memory is created, by only keeping the currently used memory areas available in main memory. With demand paging, pages are loaded into main memory on demand. The new page may replace another page, which is to be decided by a replacement strategy. The performance of paging depends on the probability of page faults:</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>âˆ’</mo><mi>p</mi><mo stretchy="false">)</mo><mo>â‹…</mo><msub><mi>t</mi><mtext>hit</mtext></msub><mo>+</mo><mi>p</mi><mo>â‹…</mo><msub><mi>t</mi><mtext>miss</mtext></msub></mrow><annotation encoding="application/x-tex">(1 - p) \cdot t_{\text{hit}} + p \cdot t_{\text{miss}}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">âˆ’</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">p</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">â‹…</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.76508em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">hit</span></span></span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.63889em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">p</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">â‹…</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.76508em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31750199999999995em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">miss</span></span></span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span></p>
<table>
<thead>
<tr>
<th>Replacement Strategy</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>First-In First-Out (FIFO)</td>
<td>Replace the page that was placed first.</td>
</tr>
<tr>
<td>Least Recently Used (LRU)</td>
<td>Replace the least recently used page. Requires control states (backwards distance)</td>
</tr>
<tr>
<td>Second Chance (clock)</td>
<td>A reference bit is set when the page is accessed. Pages are checked in FIFO order, ref: 1 -&gt; 0, 0 -&gt; replace. Possible to extend with a modification bit.</td>
</tr>
<tr>
<td>Optimal (OPT)</td>
<td>Replace the page that will be accessed farthest in the future.</td>
</tr>
</tbody>
</table>
<p>With a <strong>free page buffer</strong>, a number of free pages is always kept in memory. Pageouts take place "in advance".</p>
<p><strong>Thrashing</strong> happens when a page that was paged out is accessed immidiately after the page out happened. The process spends more time waiting to handle page faults than with its own execution. Causes of thrashing are 1) A process is close to its page maximum, 2) Too many processes in the system at the same time, 3) Suboptimal replacement strategy. Solutions to thrashing are 1) swapping of processes, 2) working set model.</p>
<h2 id="input-and-output" tabindex="-1">Input and Output</h2>
<p><a href="lectures/14.html">Link to lecture 14</a></p>
<p><strong>Character devices</strong> (such as a keyboards, printers, modems, and mice) provide a sequential stream of information. <strong>Block devices</strong> (such as disks, CD-ROM, DVD, tape drives) provide blockwise random access. Some other devices which don't fit this scheme easily are (GP)GPUs, network cards and timers.</p>
<p>Depending on the device, I/O can be performed via polling ("programmed I/O"), interrupts or DMA. <strong>Polling</strong> implies active waiting for an I/O device. <strong>Interrupts</strong> signal the software to become active, and are the source for asynchronous behavior. It implies that the CPU can be allocated to another process while waiting for a response from the device. <strong>Direct Memory Access (DMA)</strong> is used by complex controllers to transfer data from and to main memory independent of the CPU. DMA bypasses both the cache and MMU memory protection (application processes can never have direct access to program the DMA controller).</p>
<p>In Unix, peripheral devices are realized as special files. Devices are uniquely identified by a tuple: device type (block or character device), major device number (selects one specific device driver), and minor device number (selects one of multiple devices controlled by the device driver identified by the major number)</p>
<table>
<thead>
<tr>
<th>Unix access primitives</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code><span class="hljs-type">int</span> <span class="hljs-built_in">open</span>(<span class="hljs-keyword">const</span> <span class="hljs-type">char</span> *devname, <span class="hljs-type">int</span> flags)</code></td>
<td>"opens" a device and returns a file descriptor</td>
</tr>
<tr>
<td><code><span class="hljs-function"><span class="hljs-type">off_t</span> <span class="hljs-title">lseek</span><span class="hljs-params">(<span class="hljs-type">int</span> fd, <span class="hljs-type">off_t</span> offset, <span class="hljs-type">int</span> whence)</span></span></code></td>
<td>Positions the read/write pointer (relative to the start of the file) â€“ only for random access files</td>
</tr>
<tr>
<td><code><span class="hljs-function"><span class="hljs-type">ssize_t</span> <span class="hljs-title">read</span><span class="hljs-params">(<span class="hljs-type">int</span> fd, <span class="hljs-type">void</span> *buf, <span class="hljs-type">size_t</span> count)</span></span></code></td>
<td>Reads at most count bytes from descriptor fd into buffer buf</td>
</tr>
<tr>
<td><code><span class="hljs-function"><span class="hljs-type">ssize_t</span> <span class="hljs-title">write</span><span class="hljs-params">(<span class="hljs-type">int</span> fd, <span class="hljs-type">const</span> <span class="hljs-type">void</span> *buf, <span class="hljs-type">size_t</span> count)</span></span></code></td>
<td>Writes count bytes from buffer buf to file with descriptor fd</td>
</tr>
<tr>
<td><code><span class="hljs-type">int</span> <span class="hljs-built_in">close</span>(<span class="hljs-type">int</span> fd)</code></td>
<td>"closes" a device. The file descriptor fd can no longer be used after close</td>
</tr>
</tbody>
</table>
<p><strong>Buffering</strong> enables the processes and devices to operate asynchroniously. Without data buffers 1) data which arrives before a corresponding read operation is executed (e.g. keyboard input) would be discarded. 2) if an output device is busy, write would either fail or block the process until the device is ready again. 3) a process executing an I/O operation cannot be swapped.</p>
<p>With a <strong>single I/O buffer</strong> 1) The OS can accept data even if the reader process has not executed read yet 2) For block devices, a subsequent block can already be prefetched 3) he process can now be swapped, DMA writes to a buffer. 4) when writing, data is copied, the caller does not block. Data buffers in the user address space can immediately be reused.</p>
<p>With a <strong>double I/O buffer</strong> 1) While data is transferred from the I/O device to one of the buffers, the contents of the other buffer can be copied into the user address space. 2) While data is transferred from one of the buffers to the I/O device, the contents of the other buffer can already be refilled with data from the process address space.</p>
<p><strong>I/O ring buffers</strong> are commonly used. Multiple data blocks can be buffered, even if the reading process does not call read fast enough. A writer process can execute multiple write calls without being blocked.</p>
<p>I/O drivers usually queue multiple requests. Scheduling algorithms determine the order in which the requests will be handled. Some scheduling algorithms (used in mechanical disks) are First-In-First-Out (FIFO), Shortest Seek Time First (SSTF), and Elevator.</p>
<h2 id="file-systems" tabindex="-1">File Systems</h2>
<p><a href="lectures/15.html">Link to lecture 15</a></p>
<p><strong>Unix principle: "everything is a file".</strong> More precisely: 1) Every resource in the system can be accessed using a name mapped into a directory hierarchy. 2) Access to resources uses standard Unix system calls for file access. 3) File permissions are used to control access to the resource.</p>
<p>Files are identified by per process <strong>file descriptors</strong> (positive integer number) in the OS.</p>
<table>
<thead>
<tr>
<th>Unix file access API (+ virtual file system mounting)</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code><span class="hljs-type">int</span> <span class="hljs-built_in">open</span>(<span class="hljs-keyword">const</span> <span class="hljs-type">char</span> *path, <span class="hljs-type">int</span> oflag, ...)</code></td>
<td>Attempts to open the file with the given path name and options for accessing (read only, read/write etc.). Returns a file descriptor <code><span class="hljs-attribute">fd</span></code> refering to the file on success.</td>
</tr>
<tr>
<td><code><span class="hljs-function"><span class="hljs-type">ssize_t</span> <span class="hljs-title">read</span><span class="hljs-params">(<span class="hljs-type">int</span> fd, <span class="hljs-type">void</span> *buf, <span class="hljs-type">size_t</span> nbyte)</span></span></code></td>
<td>Read <code><span class="hljs-attribute">nbyte</span></code> bytes from file fd into the memory starting at user space memory address <code><span class="hljs-attribute">buf</span></code>.</td>
</tr>
<tr>
<td><code><span class="hljs-function"><span class="hljs-type">ssize_t</span> <span class="hljs-title">write</span><span class="hljs-params">(<span class="hljs-type">int</span> fd, <span class="hljs-type">const</span> <span class="hljs-type">void</span> *buf, <span class="hljs-type">size_t</span> nbyte)</span></span></code></td>
<td>Write <code><span class="hljs-attribute">nbyte</span></code> bytes to file fd from the memory starting at user space memory address <code><span class="hljs-attribute">buf</span></code>.</td>
</tr>
<tr>
<td><code><span class="hljs-type">int</span> <span class="hljs-built_in">close</span>(<span class="hljs-type">int</span> fd)</code></td>
<td>Closes the file: flushes buffers and invalidates file descriptor.</td>
</tr>
<tr>
<td><code><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">mount</span><span class="hljs-params">(<span class="hljs-type">const</span> <span class="hljs-type">char</span> *source, <span class="hljs-type">const</span> <span class="hljs-type">char</span> *target, <span class="hljs-type">const</span> <span class="hljs-type">char</span> *filesystemtype, <span class="hljs-type">unsigned</span> <span class="hljs-type">long</span> mountflags, <span class="hljs-type">const</span> <span class="hljs-type">void</span> *data)</span></span></code></td>
<td>Attaches ("mounts") a file system to the given directory in the global directory tree System Call.</td>
</tr>
<tr>
<td><code><span class="hljs-keyword">int</span> umount(<span class="hljs-keyword">const</span> <span class="hljs-keyword">char</span> *<span class="hljs-keyword">target</span>)</code></td>
<td>Removes the attachment. Note: umount, not unmount</td>
</tr>
</tbody>
</table>
<p>In most cases, files require multiple blocks of storage on disk. We simply view a disk as a large array of blocks.</p>
<p>With <strong>contigous storage</strong>, a file is stored in blocks with increasing block number. Advantages: 1) Access to all blocks with minimal delay due to disk arm positioning 2) Fast direct access to a given file offset position 3) Used for read-only file systems, e.g. CD-ROM/DVD. Disadvantages (problems): 1) Finding free space on the disk 2) Fragmentation 3) The size of new files is usually not known in advance.</p>
<p>With <strong>linked list storage</strong>, blocks of a file are linked and files can be extended and shrunk. Disadvantages are 1) Pointers take up storage space, 2) Error prone (file cannot be completely restored if the pointer information contains errors), 3) Direct access to arbitrary file positions is difficult, 4) Requires frequent positioning of the disk head.</p>
<p>Linked list storage with <strong>file allocation table (FAT)</strong> involves storing links iin separate disk blocks. Advantages: 1) Complete content of data block is usable, 2) Redundant storage of the FAT is possible (useful in case of an error). Disadvantages: 1) Additional loading of at least one block required, 2) Unused information is loaded (FAT contains links for all files), 3) Search overhead (for the data block containing information at a given offset inside a file), 4) Frequent positioning (of the disk head when data blocks are scattered over the disk)</p>
<p>With <strong>indexed storage</strong>, a special disk block contains block numbers of the data blocks of a file. A problem is that a fixed number of blocks that can be referenced in the index block. This results in fragmentation for small files and extensions required for large files.</p>
<p>With <strong>Unix inodes</strong>, storage is indexed with multiple levels. This enables the addressing of large files and fragmentation is not a problem for small files.</p>
<p><strong>Tree sequential storage</strong> (e.g. B+-tree) is used in databases to efficiently find records using a search key.</p>
<p><strong>Free space management</strong> is similar to free main memory management.</p>
<p>System crashes and defective disks are reliability challenges. The use of <strong>backups</strong> is a solution, but requires time and storage space overhead. <strong>Checksums</strong> is another, which also requires storage space overhead. Additionally, <strong>repair tools</strong> such as scandisk or fsck are able to repair (some) inconsistent metadata. However, there is a possible loss of data in the repair process, and long runtimes of the repair programs for large disks.</p>
<p>Intelligent block device (drivers) handle reliability problems below the file system layer. Advantage: all file system implementations can benefit.</p>
<p>The <strong>Unix Block Buffer Cache</strong> buffers disk blocks in main memory and uses algorithms similar to page frame handling. This enables read ahead, lazy write and free block management.</p>
<p><strong>Redundant Arrays of Inexpensive Disks (RAID)</strong> saves costs by creating large logical disks out of inexpensive smaller disks. Additional features are 1) better utilization of the available data bandwidth by using parallel transfers, 2) ault tolerance using redundancy.</p>
<table>
<thead>
<tr>
<th>RAID Level</th>
<th>Idea</th>
<th>Effect</th>
<th>Disadvantage</th>
</tr>
</thead>
<tbody>
<tr>
<td>0, Disk striping</td>
<td>Data of a large logical disk are stored in a round robin way distributed over N physical disks</td>
<td>Increased bandwidth, since multiple disks are accessed in parallel</td>
<td>Failure probability is multiplied by N</td>
</tr>
<tr>
<td>1, Disk mirroring</td>
<td>Data is stored redundantly on two disks at the same time</td>
<td>Increased read bandwidth, somewhat lower write bandwidth, higher reliability by having a copy of the data</td>
<td>Uses twice the disk space</td>
</tr>
<tr>
<td>4, Additional parity disk</td>
<td>Data is striped over multiple disks, one disk stores the related parity</td>
<td>Errors (of a single disk) can be detected and fixed without a large storage overhead. Fast read operations</td>
<td>Parity disk is bottleneck when writing</td>
</tr>
<tr>
<td>5 and 6, Distributed parity data</td>
<td>Distribute the parity block over all disks</td>
<td>Additional write overhead to update the parity block when writing is distributed</td>
<td>All data is protected with high overhead, even though a part of the data may be not critical</td>
</tr>
<tr>
<td>x+y, Hierarchies</td>
<td>Combine different RAID mechanisms in a hierarchy, e.g. RAID 1+0 (= RAID 10)</td>
<td>Properties can be combined</td>
<td>Requires a large number of disks</td>
</tr>
</tbody>
</table>
<p>In addition to writing data and metadata (e.g. inodes), <strong>journaled file systems</strong> write a protocol of the changes. At boot time, the protocol file is compared to the latest changes, this avoids inconsistencies. The log file can also be used for recovery purposes (UNDO/REDO). Advantages: 1) a transaction is either committed (completed) in whole or not at all, 2) users can define transactions that span multiple file accesses, if these are also recorded in the log, 3) impossible to create inconsistent metadata, 4) booting a crashed system only requires a fast log file check. Disadvantages: 1) less efficient, since additional log file has to be written (thus usually only metadata journaling, no full journaling).</p>
<p>With <strong>Log-structured file systems (LFS)</strong>, blocks are not overwritten, but only appended to the log. Changes are added in batches to the front, obsolete data fall out at the end. The disk bandwidth is utilized to a high degree when writing, but performance is reduced significantly if free memory is low.</p>
<p><strong>Copy-on-Write file systems (CoW)</strong> takes the idea from LFS, but is more flexible when allocation free areas.</p>
<h2 id="operating-system-architectures" tabindex="-1">Operating system architectures</h2>
<p><a href="lectures/17.html">Link to lecture 17</a></p>
<p><strong>Software architecture</strong> is defined as the basic organization of a system, expressed through its components, their relations to each other and the environment as well as the principles which define the design and evolution of the system <a href="https://gi.de/informatiklexikon/software-architektur">Source</a>.</p>
<p>A <strong>library OS</strong> is one in which the services are provided in the form of libraries and composed with the application and configuration code to construct a unikernel: a specialized, single address space <a href="https://en.wikipedia.org/wiki/Operating_system">Source</a>.</p>
<p>A <strong>monolithic kernel</strong> is an operating system architecture where the entire operating system is working in kernel space <a href="https://en.wikipedia.org/wiki/Monolithic_kernel">Source</a>.</p>
<p>A <strong>microkernel</strong> is the near-minimum amount of software that can provide the mechanisms needed to implement an OS <a href="https://en.wikipedia.org/wiki/Microkernel">Source</a>. <strong>First-generation microkernels</strong> had problems with high overhead of system calls and IPC operations, and sub-optimal decisions about which components should be implemented in the microkernel. The objective of <strong>second-generation microkernels</strong> was to remove disadvantages of first generation microkernels. "A concept is tolerated inside of a microkernel only if moving it outside of the kernel would prevent the implementation of functionality required in the system" (Jochen Liedtke: L4 (1996)).</p>
<p>The idea behind <strong>exokernels</strong> is to force as few abstractions as possible on application developers, enabling them to make as many decisions as possible about hardware abstractions. Exokernels are tiny, since functionality is limited to ensuring protection and multiplexing of resources <a href="https://en.wikipedia.org/wiki/Exokernel">Source</a>.</p>
<p>A <strong>virtual machine monitor (VMM)</strong> or <strong>hypervisor</strong> is the software component that provides the virtual machine abstraction. <strong>Hypercalls</strong> are explicit calls to the hypervisor. The objective of virtualization is isolation and multiplexing of resources below the operating system layer, and to enable simultaneous use of multiple guest operating systems.</p>
<p><strong>Paravirtualization</strong> is a virtualization technique that presents a software interface to the virtual machines which is similar, yet not identical, to the underlying hardwareâ€“software interface. The intent of the modified interface is to reduce the portion of the guest's execution time spent performing operations which are substantially more difficult to run in a virtual environment compared to a non-virtualized environment <a href="https://en.wikipedia.org/wiki/Paravirtualization">Source</a>.</p>
<table>
<thead>
<tr>
<th>Property</th>
<th>Library OS</th>
<th>Monolithic systems</th>
<th>Microkernel systems</th>
<th>Virtualization</th>
</tr>
</thead>
<tbody>
<tr>
<td>Isolation</td>
<td>Ideal â€“ single tasking system â€“ but high time overhead to switch tasks</td>
<td>No isolation of components in kernel mode, only between application processes</td>
<td>Very good â€“ separate address spaces for all components</td>
<td>Very good â€“ but coarse granularity (between VMs)</td>
</tr>
<tr>
<td>Interaction</td>
<td>Direct (function calls)</td>
<td>Direct function calls (in the kernel), Traps (application â€“ kernel)</td>
<td>Synchronous IPC</td>
<td>Communication between VMs only via TCP/IP (virtual network cards!)</td>
</tr>
<tr>
<td>Interrupts</td>
<td>Sometimes interrupts were not in use â†’ polling</td>
<td>Direct handling of hardware interrupts by IRQ handlers</td>
<td>The microkernel translates interrupts into IPC messages</td>
<td>Forwarding of IRQs to guest kernel inside of the VM (simulated hardware interrupts)</td>
</tr>
<tr>
<td>Adaptability</td>
<td>Separate libraries for each hardware architecture, no standards</td>
<td>Changes in one component influence other components</td>
<td>Originally hard to adapt â€“ x86 assembler code, today in C/C++</td>
<td>Specific adaptation for a CPU type required, paravirtualization has a lot of overhead</td>
</tr>
<tr>
<td>Extensibility</td>
<td>Depends on the library structure: global structures, "spaghetti code"</td>
<td>Originally: recompilation required; today: kernel module system</td>
<td>Very good and simple as components in user mode</td>
<td>Difficult â€“ not commonly available in VMMs</td>
</tr>
<tr>
<td>Robustness</td>
<td>Direct control of all hardware: errors â†’ system halt</td>
<td>Bad â€“ an error in one component "kills" the complete system</td>
<td>Good â€“ but dependent on the robustness of user mode components</td>
<td>Good â€“ but coarse granularity (whole VMs affected by crashes)</td>
</tr>
<tr>
<td>Performance</td>
<td>Very high due to direct operations on the hardware without privilege mechanisms</td>
<td>High â€“ few copy operations required, since all kernel components use the same address space. System calls require a trap, however</td>
<td>In general depending on the IPC performance</td>
<td>Good â€“ 5-10% lower compared to direct execution on the same hardware</td>
</tr>
</tbody>
</table>
<h3 id="cloud-operating-systems" tabindex="-1">Cloud operating systems</h3>
<p><a href="lectures/18.html">Link to lecture 18</a></p>
<p>Some <strong>cloud service models</strong> are Software-as-a-Service (SaaS), Platform-as-a-Service (PaaS), and Infrastructure-as-a-Service (IaaS).</p>
<p>Cloud-Computing challenges are 1) Data protection and privacy, 2) Vendor lock-in, 3) Quality of service.</p>
<table>
<thead>
<tr>
<th></th>
<th>Private cloud</th>
<th>Community cloud</th>
<th>Public cloud</th>
<th>Hybrid cloud</th>
</tr>
</thead>
<tbody>
<tr>
<td>Scalability</td>
<td>restricted</td>
<td>restricted</td>
<td>very high</td>
<td>very high</td>
</tr>
<tr>
<td>Security</td>
<td>most secure</td>
<td>very secure</td>
<td>moderately secure</td>
<td>very secure</td>
</tr>
<tr>
<td>Performance</td>
<td>very good</td>
<td>very good</td>
<td>low to medium</td>
<td>good</td>
</tr>
<tr>
<td>Reliability</td>
<td>very high</td>
<td>very high</td>
<td>medium</td>
<td>medium to high</td>
</tr>
<tr>
<td>Costs</td>
<td>high</td>
<td>medium</td>
<td>low</td>
<td>medium</td>
</tr>
</tbody>
</table>
<p>With <strong>container-based virtualization</strong>, the OS kernel is virtualized and the containers share a kernel.</p>
<p>With <strong>hardware virtualization</strong>, a complete computer (CPU, memory, I/O devices) is virtualized.</p>
<h2 id="security" tabindex="-1">Security</h2>
<p><a href="lectures/21.html">Link to lecture 21</a></p>
<p><strong>Safety</strong> is protection against risks due to hardware and software errors or failures. <strong>Security</strong> is protection of users and computers against intended errors (attacks).</p>
<p>Someone (differentiation of persons and groups) has to be deterred from doing (using technical and organizational methods) some unexpected things! 1) unauthorized reading of data (secrecy, confidentiality), 2) unauthorized writing of data (integrity), 3) working under a "false flag" (authenticity), 4) unauthorized use of resources (availability).</p>
<p>Example: <a href="https://en.wikipedia.org/wiki/Login_spoofing">login spoofing</a>. A fake login screen may be used to access the user credentials. Solution: require the user to start the login sequence using a key combination that cannot be intercepted by a user program.</p>
<p>Example: <strong>virus</strong>. Program code inserted into another program, start of the infected program results in virus reproduction. Sorts of viruses: 1) Boot sector virus: executed at system startup time. 2) Macro virus: in scriptable programs, e.g. Word, Excel (Reproduced through documents e.g. sent by email)! 3)
Executable program as virus.</p>
<p>Example: <strong>social engineering</strong>. Gain access to information by exploiting human errors. Phishing is to obtain data of an internet user by pretending to be someone else (with forged addresses). Pharming is manipulation of DNS requests by web browsers, redirecting accesses, e.g. to forged bank websites.</p>
<p>Some real malware examples are the Unix Morris worm, the Michelangelo virus, the Sony BMG root kit and the Blue Pill.</p>
<table>
<thead>
<tr>
<th>Malware</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Viruses</td>
<td>programs inadvertently distributed by a user</td>
</tr>
<tr>
<td>Worms</td>
<td>do not wait for user actions to propagate to another computer</td>
</tr>
<tr>
<td>Trojan horses</td>
<td>program disguised as useful application</td>
</tr>
<tr>
<td>Root kit</td>
<td>collection of software tools to disguise future logins of an attacker and hide processes and files, installed after a computer system is compromised</td>
</tr>
</tbody>
</table>
<p><strong>Design principes of permission management</strong> are 1) Principle of least privilege (Allow a person or software component only those permissions that are required for the functionality to be realized) 2) Fail-safe defaults 3) Separation of duties.</p>
<p>An <strong>access matrix</strong> consists of Subjects (persons/users, processes, Objects (data, devices, processes, memory) and Operations (read, write, delete, execute). It is used to determine if <code><span class="hljs-function"><span class="hljs-title">operation</span><span class="hljs-params">(subject, object)</span></span></code> is permitted.</p>
<p>Some access matrix variants are 1) <strong>Access Control Lists (ACL / Coloumns)</strong>: permissions are validated based on the identity of the requesting subject (user). 2) <strong>Capabilities (Rows)</strong>: for every access to an object a property is validated which is owned by the subject and which can be passed to other subjects on demand. 3) <strong>Mandatory access control (Rule-based)</strong>: rules are evaluated for every access.</p>
<p>Unix uses simple Access Control Lists (ACL). Processes have a user ID and a group ID, files have an owner and a group. Permissions are related to the user (owner), group, and all others.</p>
<p>The Memory Management Unit (MMU) provides <strong>hardware-based protection</strong> by 1) only mapping the exact set of required main memory pages into the virtual address space of a process, 2) isolation of the physical address spaces of different processes, 3) protection bits for each page, controlled at every access.</p>
<p><strong>Protection rings</strong> is a privilege concept where all code is executed in the context of a given ring, where the innermost ring (0) has access to all system resources. User programs usually run in ring 3.</p>
<p><strong>Software bugs</strong> such as exceeding a value range or a heap/buffer overflow are security issues, and can be exploited.</p>

        <div style="height: 100vh;"></div>
      </article>
      </div>
  </body>
</html>
