<!DOCTYPE html>
<html>
  <head>
    <!-- Katex -->
    <link rel="stylesheet" href=
        "https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"/>

    <!-- GitHub Markdown Styles -->
    <link rel="stylesheet" href=
        "https://cdn.jsdelivr.net/github-markdown-css/2.2.1/github-markdown.css"/>

    <title>16.md</title>
    <link rel="icon" type="image/x-icon" href="../../../favicon.png"/>

    <!-- Custom Styles -->
    <link rel="stylesheet" href="../../../styles.css">
  
  </head>

  <body class="markdown-body">
    <div class="page flex-row">
      <div class="col links">
        
<p><h4><a href="../index.html">tdt4186/</a><a href="./index.html">lectures</a>
</h4></p>
<ul>
<li>üìÇ <a href="./assets/index.html">assets</a></li>
<li>üìÑ <a href="01.html">01</a></li>
<li>üìÑ <a href="02.html">02</a></li>
<li>üìÑ <a href="03.html">03</a></li>
<li>üìÑ <a href="04.html">04</a></li>
<li>üìÑ <a href="05.html">05</a></li>
<li>üìÑ <a href="06.html">06</a></li>
<li>üìÑ <a href="07.html">07</a></li>
<li>üìÑ <a href="08.html">08</a></li>
<li>üìÑ <a href="09.html">09</a></li>
<li>üìÑ <a href="10.html">10</a></li>
<li>üìÑ <a href="11.html">11</a></li>
<li>üìÑ <a href="12.html">12</a></li>
<li>üìÑ <a href="13.html">13</a></li>
<li>üìÑ <a href="14.html">14</a></li>
<li>üìÑ <a href="15.html">15</a></li>
<li>üìÑ <a href="16.html">16 ‚ú®</a></li>
<li>üìÑ <a href="17.html">17</a></li>
<li>üìÑ <a href="18.html">18</a></li>
<li>üìÑ <a href="19.html">19</a></li>
<li>üìÑ <a href="20.html">20</a></li>
<li>üìÑ <a href="21.html">21</a></li>
<li>üìÑ <a href="22.html">22</a></li>
<li>üìÑ <a href="questions.html">questions</a></li>
</ul>
<p><h4>Table of Contents</h4></p>
<nav class="table-of-contents"><ol><li><a href="#lecture-16%3A-modern-file-systems">Lecture 16: Modern file systems</a><ol><li><a href="#exam">Exam</a></li><li><a href="#what-do-we-know-about-storage-so-far%3F">What do we know about storage so far?</a></li><li><a href="#challenge%3A-reliability-(1)">Challenge: Reliability (1)</a></li><li><a href="#challenge%3A-reliability-(2)">Challenge: Reliability (2)</a></li><li><a href="#challenge%3A-performance-optimization">Challenge: Performance optimization</a></li><li><a href="#challenge%3A-disk-management">Challenge: Disk management</a></li><li><a href="#intelligent-block-device-(drivers)">Intelligent block device (drivers)</a></li><li><a href="#unix-block-buffer-cache">UNIX Block Buffer Cache</a></li><li><a href="#unix-block-buffer-cache-(2)">UNIX Block Buffer Cache (2)</a></li><li><a href="#unix-block-buffer-cache%3A-structure">UNIX Block Buffer Cache: Structure</a></li><li><a href="#unix-block-buffer-cache%3A-structure-(2)">UNIX Block Buffer Cache: Structure (2)</a></li><li><a href="#%5Blinux%5D-logical-volume-management">[Linux] Logical Volume Management</a></li><li><a href="#redundant-arrays-of-inexpensive-disks">Redundant Arrays of Inexpensive Disks</a></li><li><a href="#raid-0%3A-disk-striping">RAID 0: Disk striping</a></li><li><a href="#raid-1%3A-disk-mirroring">RAID 1: Disk mirroring</a></li><li><a href="#raid-4%3A-additional-parity-disk">RAID 4: Additional parity disk</a></li><li><a href="#raid-5-and-6%3A-distributed-parity-data">RAID 5 and 6: Distributed parity data</a></li><li><a href="#raid-x%2By-(%3D-raid-xy)%3A-hierarchies">RAID x+y (= RAID xy): Hierarchies</a></li><li><a href="#journaled-file-systems">Journaled File Systems</a></li><li><a href="#journaled-file-systems%3A-protocol">Journaled File Systems: Protocol</a></li><li><a href="#journaled-file-systems%3A-recovery">Journaled File Systems: Recovery</a></li><li><a href="#journaled-file-systems%3A-results">Journaled File Systems: Results</a></li><li><a href="#log-structured-file-systems-%5B1%5D">Log-structured file systems [1]</a></li><li><a href="#log-structured-file-systems-(2)">Log-structured file systems (2)</a></li><li><a href="#cow%3A-copy-on-write-file-systems">CoW: Copy-on-Write file systems</a></li><li><a href="#btrfs%3A-%E2%80%9Cbutter%E2%80%9D-fs-%5B2%5D">BTRFS: ‚Äúbutter‚Äù FS [2]</a></li><li><a href="#conclusion">Conclusion</a></li></ol></li></ol></nav>
      </div>
      <article class="col content">
        
<h1 id="lecture-16%3A-modern-file-systems" tabindex="-1">Lecture 16: Modern file systems</h1>
<p><a href="15.html">Previous lecture</a>
<a href="17.html">Next lecture</a></p>
<p><iframe
width="560" height="315" src="https://www.youtube.com/embed/lfHEBzFOhqM"
title="YouTube video player"
frameborder="0"
allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
allowfullscreen>
</iframe></p>
<h2 id="exam" tabindex="-1">Exam</h2>
<p>Modern file systems (FS)</p>
<p><strong>Important questions:</strong></p>
<ul>
<li>What are the challenges for file systems today?</li>
<li>How can the reliability of disk storage be improved?</li>
<li>How can the performance of disk storage be improved?</li>
<li>What is the Unix block buffer cache and how does it work?</li>
<li>What is logical volume management and why is it useful?</li>
<li>What is RAID?
<ul>
<li>Which different RAID levels exist and how do they work?</li>
<li>Can you discuss the pros/cons of the different levels?</li>
</ul>
</li>
<li>What is a journaling/log structured file system?
<ul>
<li>How do they work, what are differences to traditional FS?</li>
</ul>
</li>
</ul>
<h2 id="what-do-we-know-about-storage-so-far%3F" tabindex="-1">What do we know about storage so far?</h2>
<ul>
<li>Disk drives have a block structure and provide random access
<ul>
<li>Locality of disk accesses is crucial to enable high performance
<ul>
<li>head movements are especially costly ‚Äì multiple ms</li>
</ul>
</li>
<li>SSDs are also block structured but have no mechanical delays</li>
</ul>
</li>
<li>File systems provide abstractions to enable programs to work with persistent data
<ul>
<li>Files and directory hierarchies</li>
<li>Metadata, e.g. name, size, file creation data, ‚Ä¶</li>
</ul>
</li>
<li>There are different ways to map file systems onto a disk</li>
</ul>
<h2 id="challenge%3A-reliability-(1)" tabindex="-1">Challenge: Reliability (1)</h2>
<ul>
<li>Problems:
<ul>
<li>Defective disks or blocks</li>
<li>System crashes or failures</li>
</ul>
</li>
<li>Impacts:
<ul>
<li>Complete loss of data</li>
<li>Defective data blocks, e.g.
<ul>
<li>application can no longer read a file</li>
</ul>
</li>
<li>Inconsistent metadata, e.g.
<ul>
<li>directory entry for a file is missing or vice versa</li>
<li>block is used but marked as free</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img src="assets/lecture.16.bath_tub_curve.png" alt="">
The "bath tub curve" shows the typical development of the error rate of hard disks (and most other technical products) over their lifetime</p>
<h2 id="challenge%3A-reliability-(2)" tabindex="-1">Challenge: Reliability (2)</h2>
<ul>
<li>Solution: Backup
<ul>
<li>Frequent incremental and complete backup of data to a different storage medium</li>
<li>Problems: time and storage space overhead</li>
</ul>
</li>
<li>Solution: Checksums
<ul>
<li>Files can be annotated with a checkum (error detection) or an error-correcting code (repair)</li>
<li>Problems: storage space overhead; responsibility (layer)</li>
</ul>
</li>
<li>Solution: Repair tools
<ul>
<li>Programs such as chkdsk, scandisk or fsck are able to repair (some) inconsistent metadata</li>
<li>Problems: Possible loss of data in the repair process; long runtimes of the repair programs for large disks</li>
</ul>
</li>
</ul>
<h2 id="challenge%3A-performance-optimization" tabindex="-1">Challenge: Performance optimization</h2>
<ul>
<li>Problem:
<ul>
<li>Hard disks have low read/write speeds and a high positioning latency</li>
<li>CPU/main memory performance and disk performance diverge</li>
<li>Impact:</li>
<li>The hard disk becomes the bottleneck for I/O intensive applications (e.g. databases) and tasks (e.g. booting the system or starting a program)</li>
</ul>
</li>
<li>Solution: Cache
<ul>
<li>Keep important (meta)data in main memory</li>
<li>Problem: Consistency between cache and disk</li>
</ul>
</li>
</ul>
<h2 id="challenge%3A-disk-management" tabindex="-1">Challenge: Disk management</h2>
<ul>
<li>Problem:
<ul>
<li>Physical dimensions of disk drives limit the size of the file system(s)</li>
<li>What can be done if a disk is full? Impact:</li>
<li>Disk capacity is over dimensioned to avoid the overhead of copying to a different disk</li>
</ul>
</li>
<li>Solution: Virtual file system
<ul>
<li>Mount new disks as directories (using "soft links")</li>
<li>Problems: not transparent for users and applications; size limitation still in place for existing directories</li>
</ul>
</li>
</ul>
<p><img src="assets/lecture.16.disk_management.png" alt=""></p>
<h2 id="intelligent-block-device-(drivers)" tabindex="-1">Intelligent block device (drivers)</h2>
<ul>
<li>Idea: Handle reliability problems below the file system layer</li>
<li>Advantage: all file system implementations can benefit</li>
</ul>
<p><img src="assets/lecture.16.intelligent_block_device.png" alt=""></p>
<h2 id="unix-block-buffer-cache" tabindex="-1">UNIX Block Buffer Cache</h2>
<ul>
<li>Buffer for disk blocks in main memory
<ul>
<li>Uses algorithms similar to page frame handling</li>
<li>Read ahead: for sequential reads, the transfer of subsequent data blocks is initiated</li>
<li>Lazy write: a block is not written to disk directly</li>
<li>allows optimization of write accesses and does not block the writer</li>
<li>Free block management in a free list
<ul>
<li>Possible entries for the free list are determined using LRU</li>
<li>Blocks which are already marked free but are not yet reused can be reactivated (reclaim)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="unix-block-buffer-cache-(2)" tabindex="-1">UNIX Block Buffer Cache (2)</h2>
<ul>
<li>Write to disk if/when
<ul>
<li>no more free buffers are available</li>
<li>periodically by the system (fsflush process, update process),</li>
<li>when calling the sync(2) system call</li>
<li>and after each write system call when the corresponding file was opened with the option O_SYNC</li>
</ul>
</li>
<li>Adressing
<ul>
<li>Blocks are addressed using a tuple:
<ul>
<li>(device number, block number)</li>
</ul>
</li>
<li>A hash of the address is used to select one of the possible buffer lists</li>
</ul>
</li>
</ul>
<h2 id="unix-block-buffer-cache%3A-structure" tabindex="-1">UNIX Block Buffer Cache: Structure</h2>
<p><img src="assets/lecture.16.unix_block_buffer_cache_structure.png" alt=""></p>
<h2 id="unix-block-buffer-cache%3A-structure-(2)" tabindex="-1">UNIX Block Buffer Cache: Structure (2)</h2>
<p><img src="assets/lecture.16.unix_block_buffer_cache_structure_2.png" alt=""></p>
<h2 id="%5Blinux%5D-logical-volume-management" tabindex="-1">[Linux] Logical Volume Management</h2>
<ul>
<li>1:1 relation between file system and disk is no longer enforced</li>
</ul>
<p><img src="assets/lecture.16.linux_logical_volume_management.png" alt=""></p>
<h2 id="redundant-arrays-of-inexpensive-disks" tabindex="-1">Redundant Arrays of Inexpensive Disks</h2>
<p>(short: RAID)</p>
<ul>
<li>Initial idea: save costs by creating large logical disks out of inexpensive smaller disks (cost per GB)</li>
<li>Additional features:
<ul>
<li>better utilization of the available data bandwidth by using parallel transfers</li>
<li>fault tolerance using redundancy</li>
</ul>
</li>
<li>Two variants:
<ul>
<li>Hardware RAID: disk controller with special management software (+potentially cache)</li>
<li>Software RAID: layer between disk driver and file system code</li>
</ul>
</li>
</ul>
<h2 id="raid-0%3A-disk-striping" tabindex="-1">RAID 0: Disk striping</h2>
<ul>
<li>Idea: Data of a large logical disk are stored in a round robin way distributed over N physical disks:</li>
<li>Effect: increased bandwidth, since multiple disks are accessed in parallel</li>
<li>Disadvantage: failure probability is multiplied by N</li>
</ul>
<p><img src="assets/lecture.16.raid_0.png" alt=""></p>
<h2 id="raid-1%3A-disk-mirroring" tabindex="-1">RAID 1: Disk mirroring</h2>
<ul>
<li>Idea: data is stored redundantly on two disks at the same time:</li>
<li>Effect: increased read bandwidth, somewhat lower write bandwidth, higher reliability by having a copy of the data</li>
<li>Disadvantage: uses twice the disk space</li>
</ul>
<p><img src="assets/lecture.16.raid_1.png" alt=""></p>
<h2 id="raid-4%3A-additional-parity-disk" tabindex="-1">RAID 4: Additional parity disk</h2>
<ul>
<li>Idea: data is striped over multiple disks, one disk stores the related parity</li>
<li>Effect: errors (of a single disk) can be detected and fixed without a large storage overhead. Fast read operations</li>
<li>Disadvantage: parity disk is bottleneck when writing</li>
</ul>
<p><img src="assets/lecture.16.raid_4.png" alt=""></p>
<h2 id="raid-5-and-6%3A-distributed-parity-data" tabindex="-1">RAID 5 and 6: Distributed parity data</h2>
<ul>
<li>Idea: distribute the parity block over all disks</li>
<li>Effect: additional write overhead to update the parity block when writing is distributed
<ul>
<li>With RAID 6, an additional parity block can be used to restore the data of two failed disks</li>
</ul>
</li>
<li>Disadvantage: all data is protected with high overhead, even though a part of the data may be not critical</li>
</ul>
<p><img src="assets/lecture.16.raid_5_6.png" alt=""></p>
<h2 id="raid-x%2By-(%3D-raid-xy)%3A-hierarchies" tabindex="-1">RAID x+y (= RAID xy): Hierarchies</h2>
<ul>
<li>Idea: Combine different RAID mechanisms in a hierarchy, e.g. RAID 1+0 (= RAID 10):</li>
<li>Effect: properties can be combined. Common setups: RAID 10, 50 or 60</li>
<li>Disadvantage: requires a large number of disks</li>
</ul>
<p><img src="assets/lecture.16.raid_x_y.png" alt=""></p>
<h2 id="journaled-file-systems" tabindex="-1">Journaled File Systems</h2>
<ul>
<li>In addition to writing data and metadata (e.g. inodes), journaled file systems write a protocol of the changes
<ul>
<li>All changes are part of a transaction</li>
<li>Examples for transactions:
<ul>
<li>create, delete, expand, shorten files</li>
<li>change file attributes</li>
<li>rename a file</li>
</ul>
</li>
<li>All changes to the file system are additionally stored in a protocol file (log file)</li>
<li>At boot time, the protocol file is compared to the latest changes, this avoids inconsistencies</li>
</ul>
</li>
</ul>
<h2 id="journaled-file-systems%3A-protocol" tabindex="-1">Journaled File Systems: Protocol</h2>
<ul>
<li>A protocol entry is generated for each single operation of a transaction and‚Ä¶
<ul>
<li>after this, the change to a file system is carried out</li>
</ul>
</li>
<li>Important conditions:
<ul>
<li>A protocol entry is always written to disk before the change itself</li>
<li>If something was changed on a disk, the related protocol entry is also found on that disk</li>
</ul>
</li>
</ul>
<h2 id="journaled-file-systems%3A-recovery" tabindex="-1">Journaled File Systems: Recovery</h2>
<ul>
<li>When booting a system, the operating system checks, if the changes logged in the protocol are present on disk:</li>
<li>A transaction can be repeated or committed if all protocol entries are available on disk ‚Üí redo</li>
<li>Started transactions that have not been completed are revoked ‚Üí undo</li>
</ul>
<h2 id="journaled-file-systems%3A-results" tabindex="-1">Journaled File Systems: Results</h2>
<ul>
<li>Advantages:
<ul>
<li>a transaction is either committed (completed) in whole or not at all</li>
<li>users can define transactions that span multiple file accesses, if these are also recorded in the log</li>
<li>impossible to create inconsistent metadata</li>
<li>booting a crashed system only requires a fast log file check
<ul>
<li>the alternative chkdsk takes a long time for large disks</li>
</ul>
</li>
</ul>
</li>
<li>Disadvantages:
<ul>
<li>less efficient, since additional log file has to be written</li>
<li>thus usually only metadata journaling, no full journaling</li>
<li>examples: Windows NTFS, Linux ext 3/4, IBM JFS</li>
</ul>
</li>
</ul>
<h2 id="log-structured-file-systems-%5B1%5D" tabindex="-1">Log-structured file systems [1]</h2>
<p>(short: LFS)</p>
<ul>
<li>Observation:
<ul>
<li>Large caches reduce the frequency of read operations</li>
<li>Write operations should not be scattered</li>
</ul>
</li>
<li>(Radical) approach: one log is sufficient for everything!
<ul>
<li>Blocks are not overwritten, but only appended to the log</li>
<li>Changes to metadata are also stored in the log only</li>
<li>Write operations are collected in main memory and then written to disk as a single large segment (e.g. 1 MB)</li>
<li>Only the superblock has a fixed position on the disk</li>
</ul>
</li>
</ul>
<h2 id="log-structured-file-systems-(2)" tabindex="-1">Log-structured file systems (2)</h2>
<ul>
<li>Example:</li>
</ul>
<p><img src="assets/lecture.16.lfs.png" alt=""></p>
<ul>
<li>Log works like a ring buffer: changes are added to the front, obsolete data fall out at the end</li>
<li>Cleaner: process to compactify/release segments</li>
<li>Effect:
<ul>
<li>Consistency: new segments are entirely visible or not at all</li>
<li>The disk bandwidth is also utilized to a high degree when writing</li>
<li>Performance reduced significantly if free memory is low</li>
</ul>
</li>
</ul>
<h2 id="cow%3A-copy-on-write-file-systems" tabindex="-1">CoW: Copy-on-Write file systems</h2>
<ul>
<li>Many modern file systems refrain from overwriting
<ul>
<li>Idea from LFS, but more flexible when allocation free areas</li>
</ul>
</li>
<li>Example: manipulate file (B+ tree)‚Ä¶ [2]</li>
</ul>
<p><img src="assets/lecture.16.cow.png" alt=""></p>
<ul>
<li>Example: "copy" complete directory tree</li>
</ul>
<p><img src="assets/lecture.16.cow_2.png" alt=""></p>
<h2 id="btrfs%3A-%E2%80%9Cbutter%E2%80%9D-fs-%5B2%5D" tabindex="-1">BTRFS: ‚Äúbutter‚Äù FS [2]</h2>
<p>‚Ä¶ according to developer Chris Mason ("comes from a CoW")</p>
<ul>
<li>Widely used on Linux, inspired by Sun ZFS</li>
<li>Features: ‚Ä¶very many‚Ä¶
<ul>
<li>Fast writes: Special "CoW friendly" B+ trees</li>
<li>Resource-saving snapshots</li>
<li>No loss of data
<ul>
<li>Atomic changes and checksums for all metadata and data</li>
</ul>
</li>
<li>Use of multiple disks
<ul>
<li>Implements flexible RAID: differentiates between data and metadata</li>
</ul>
</li>
<li>Size changes while the system is running</li>
<li>Data compression</li>
</ul>
</li>
</ul>
<h2 id="conclusion" tabindex="-1">Conclusion</h2>
<ul>
<li>Modern file systems‚Ä¶
<ul>
<li>consider the properties of current hardware: large main memories (cache), fast parallel CPU cores, ‚Ä¶</li>
<li>have many new features: snapshots, volume management, redundancy, ‚Ä¶</li>
</ul>
</li>
<li>Basic design decision: Should this functionality be implemented in the file system (or rather at a lower layer)?
<ul>
<li>Pro:
<ul>
<li>more flexibility</li>
<li>possible to make use of knowledge about the file system structure, e.g. different RAID levels for data and metadata</li>
</ul>
</li>
<li>Con:
<ul>
<li>All file systems would benefit from functionality implemented on the driver level</li>
</ul>
</li>
</ul>
</li>
</ul>

        <div style="height: 100vh;"></div>
      </article>
      </div>
  </body>
</html>
