<!DOCTYPE html>
<html>

<head>
  <!-- Katex -->
  <link rel="stylesheet" href=
      "https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css">

  <!-- GitHub Markdown Styles -->
  <link rel="stylesheet" href=
      "https://cdn.jsdelivr.net/github-markdown-css/2.2.1/github-markdown.css"/>

  <title>kompendium.md</title>
  <link rel="icon" type="image/x-icon" href="../../null">

  <!-- Custom Styles -->
  <link rel="stylesheet" href="../../styles.css">
  
</head>

<body>
<article class="markdown-body">

<p>↩️ <a href="./index.html">tdt4186</a></p>
<h1>TDT4186 Kompendium</h1>
<h1>Notes</h1>
<p>Operating systems know the hardware in detail and provides suitable abstractions, to serve the users and their application programs.</p>
<h4>OS Development</h4>
<p>The first system softwares were reusable probram libraries: linkers, loaders, debuggers and device drivers. The first operating systems were "resident monitors"; they handled interpretation of job control commands, loading and execution of programs and device control.</p>
<p>With multiprogramming, the CPU works on multiple jobs at the same time. In consequence, the OS must handle concurrent I/O activites,
main memory for multiple programs, programs in execution (processes), processor scheduling and multi user operation (security and accounting).</p>
<h4>Memory management</h4>
<p>Access to I/O devices is often slow ("memory gap", "I/O bottleneck"). Polling sends a command and then waits until the device returns data. With interrupts, the device notifies the program when data is ready.</p>
<p>Caches work efficiently due to two locality principles:</p>
<ul>
<li>Temporal locality: a program accessing some part of memory is likely to access the same memory soon thereafter</li>
<li>Spatial locality: a program accessing some part of memory is likely to access nearby memory next</li>
</ul>
<p>The MMU translates allowed ("virtual", "logical") addresses to "physical" addresses in main memory using a translation table. Enables security by separating process address spaces. The MMU uses a special cache on the CPU - the translation lookaside buffer (TLB).</p>
<p>Memory management involves address mapping (logical to physical) and placement/replacement strategies.</p>
<p>An access matrix consists of Subjects (persons/users, processes, Objects (data, devices, processes, memory) and Operations (read, write, delete, execute). It is used to determine if <code><span class="hljs-function"><span class="hljs-title">operation</span><span class="hljs-params">(subject, object)</span></span></code> is permitted.</p>
<h4>Shells and pipes</h4>
<p>Unix shells (a "shell" around the operating system "core") are text based user interface to start commands (Unix programs). Every executed command is a separate child process.</p>
<p>Standard I/O channels (stdin, stdout, stderr) are usually connected to the terminal in which the shell runs that started the process. The numerical file descriptors assigned to these channels are 0 to stdin, 1 to stdout and 2 to stderr. <code><span class="hljs-meta">&gt;</span></code> redirects standard output, <code>&lt;</code> redirects standard input and <code><span class="hljs-string">|</span></code> (pipe) symbol tells the shell to connect the standard output of the left process to the standard input of the right process.</p>
<p>Doug Mcllroy, the inventor of Unix pipes, described the <a href="https://en.wikipedia.org/wiki/Unix_philosophy">Unix philosophy</a>. It can be summarized as "Do one thing, do it well."</p>
<h4>Processes</h4>
<p>A process is a program in execution. They often consist of alternating sequences of "CPU bursts" and "I/O bursts". The process context consists of</p>
<ul>
<li>Memory: code, data, and stack segment (text, data, bss, stack, heap)</li>
<li>Contents on processor registers (Instruction pointer, Stack pointer, General purpose registers)</li>
<li>Process state (RUNNING, READY, BLOCKED, etc.)</li>
<li>User ID (and group ID)</li>
<li>Access permissions</li>
<li>Currently used resources (Files, I/O devices, etc.)</li>
</ul>
<p>Scheduling enables the coordination of concurrent processes. Scheduling algorithms can be user oriented (short reaction times)or system oriented (optimal CPU utilization).</p>
<p>Inter-process communication (IPC) enables the collaboration of multiple processes. Examples are shared memory and message passing.</p>
<p>From the point of view of the application, calling an operating system service looks like a regular function call, e.g.: <code><span class="hljs-attribute">pid</span> <span class="hljs-operator">=</span> fork()<span class="hljs-comment">;</span></code> (the C library (libc) provides stubs (adapter functions) that call the actual syscall). However, arbitrarily calling code inside the OS kernel is dangerous. Many CPUs provide several execution modes: "user mode": only restricted functionality is allowed, and "kernel" or "supervisor mode": full access to all hardware resources.</p>
<table>
<thead>
<tr>
<th>Syscall</th>
<th>Description</th>
<th>Manual section</th>
</tr>
</thead>
<tbody>
<tr>
<td>getpid</td>
<td>returns PID of the calling process</td>
<td>(2)</td>
</tr>
<tr>
<td>getppid</td>
<td>returns PID of the parent process</td>
<td>(2)</td>
</tr>
<tr>
<td>getuid</td>
<td>return the UID of the calling process</td>
<td>(2)</td>
</tr>
<tr>
<td>fork</td>
<td>creates a new child process</td>
<td>(2)</td>
</tr>
<tr>
<td>exit</td>
<td>terminates the calling process</td>
<td>(3)</td>
</tr>
<tr>
<td>_exit</td>
<td>terminates the calling process</td>
<td>(2)</td>
</tr>
<tr>
<td>wait</td>
<td>waits for the termination of a child process</td>
<td>(2)</td>
</tr>
<tr>
<td>execve</td>
<td>loads and starts a program in the context of the calling process</td>
<td>(2)</td>
</tr>
</tbody>
</table>
<p>Read Unix manual pages with <code>man <span class="hljs-tag">&lt;<span class="hljs-name">num</span>&gt;</span> <span class="hljs-tag">&lt;<span class="hljs-name">command</span>&gt;</span></code></p>
<p>With <code><span class="hljs-function"><span class="hljs-type">pid_t</span> <span class="hljs-title">fork</span><span class="hljs-params">(<span class="hljs-type">void</span>)</span></span></code>, the child's PID is returned to the parent process, and 0 is returned to the child process. The child process continues the program from the current line and inherits most parent process attributes (copy on write), but not the Process ID (PID), and parent process ID (PPID). Copy on write is a an optimization which only really copies the memory if it is updated (written to).</p>
<p><code><span class="hljs-keyword">void</span> <span class="hljs-title function_">_exit</span><span class="hljs-params">(<span class="hljs-type">int</span>)</span></code> terminates the calling process and passes an integer argument as "exit status" to the parent process. It also releases the resources allocated by the process. In C, the library function <code><span class="hljs-keyword">exit</span>()</code> should be used, shich additionally releases resources used by the C library.</p>
<p><code><span class="hljs-function"><span class="hljs-type">pid_t</span> <span class="hljs-title">wait</span><span class="hljs-params">(<span class="hljs-type">int</span> *)</span></span></code> blocks the calling process until one of its child processes terminates, or returns immediately if all child processes are already terminated. The return value is the terminated child's PID. Using the <code><span class="hljs-built_in">int</span> *</code> parameter, the caller is passed the child's "exit status".</p>
<p>A terminated process is called a "zombie" until its exit status is requested using wait. The resources allocated to such processes can be released, but the OS project management still needs to know about them (i.e. exit status has to be saved).</p>
<p>If a parent process terminates before a child, the child process is orphaned. The init process (PID 1) adopts all orphaned processes.</p>
<p><code><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">execve</span><span class="hljs-params">(<span class="hljs-type">const</span> <span class="hljs-type">char</span> *command, <span class="hljs-type">const</span> <span class="hljs-type">char</span> *args[], <span class="hljs-type">const</span> <span class="hljs-type">char</span> *encp[])</span></span></code> loads and starts the command passed in, remaining in the same process. Alternatives: <code><span class="hljs-attribute">execl</span></code>, <code><span class="hljs-attribute">execv</span></code>, <code><span class="hljs-attribute">execlp</span></code>, <code><span class="hljs-attribute">execvp</span></code>.</p>
<p>A more complete unix process state diagram:
<img src="assets/2022-05-18-11-41-31.png" alt=""></p>
<p>Traditional Unix process creation using fork is too heavyweight for some applications.</p>
<h4>Threads and Fibers</h4>
<p>Threads are lightweight (usually kernel-level) processes and can share address space (code + data + bss + heap). Advantages: Faster context switching, complex operations can run in parallel of user I/O. Disadvantages: error-prone, shared access requires coordination, scheduling overhead.</p>
<p>Fibers are also called user-level threads (or green threads, or featherweight processes). Implemented on application layer, unknown to the OS. Advantages: 1) Extremely fast context switch: only exchange processor registers. 2) No switch to kernel mode required to switch to different fiber 3) Every application can choose the fiber library best suited for it Disadvantages: 1) Blocking a single fiber leads to blocking the whole process (since the OS doesn't know about fibers). 2) No speed advantage from multiprocessor systems.</p>
<table>
<thead>
<tr>
<th></th>
<th>Processes</th>
<th>Threads</th>
<th>Fibers</th>
</tr>
</thead>
<tbody>
<tr>
<td>Address space</td>
<td>separate</td>
<td>common</td>
<td>common</td>
</tr>
<tr>
<td>Kernel visibility</td>
<td>yes</td>
<td>yes</td>
<td>no</td>
</tr>
<tr>
<td>Scheduling</td>
<td>kernel level</td>
<td>kernel level</td>
<td>user space</td>
</tr>
<tr>
<td>Stack</td>
<td>separate per process</td>
<td>separate per thread</td>
<td>can be common</td>
</tr>
<tr>
<td>Switching overhead</td>
<td>very high</td>
<td>high</td>
<td>low</td>
</tr>
</tbody>
</table>
<p>Windows processes provide environment and address space for threads. Every thread has its own stack and CPU register set. User level threads (fibers) are possible, but unusual. Strategy: keep the number of threads low; Use overlapping (asynchronous) I/O.</p>
<p>Linux implements POSIX threads using the pthreads library. All threads and processes are internally managed as tasks, which the scheduler does not differentiate between.</p>
<h4>Synchronization</h4>
<p>A race condition is a situation in which multiple processes access shared data concurrently and at least one of the processes manipulates the data. To avoid race conditions, concurrent processes need to be synchronized (coordinated).</p>
<p>Only a single process can be in a critical section at the same time. Solved by mutual exclusion, using the mutex (<a href="https://en.wikipedia.org/wiki/Lock_(computer_science)">lock</a>) abstraction. However, deadlocks must be considered.</p>
<p>The bakery algorithm is a working solution for the problem of critical sections.</p>
<p>Another idea to ensure that a process stays in the critical section is to suppress interrupts (because they cause context switches).</p>
<p>Many CPUs support indivisible (atomic) read/modify/write cycles that can be used to implement lock algorithms.</p>
<p>Semaphores are an operating system abstraction to exchange synchronization signals between concurrent processes. A semaphore is defined as a non-negative integer with two atomic operations: <code><span class="hljs-built_in">wait</span></code> (decrement) and <code><span class="hljs-keyword">signal</span><span class="hljs-string"></span></code> (increment).</p>
<p>A monitor is an abstract data type with implicit synchronization properties. They are for example implemented in Java.</p>
<p>An actively waiting process 1) is unable to change the condition it is waiting for on its own 2) unnecessarily impedes other processes which would be able to use the CPU for "useful" work. 3) harms itself: the longer a process holds the processor, the longer it has to wait for other processes to fulfill the condition it is waiting for.</p>
<p>In the case of passive waiting, the process is entered into a waiting queue and is not unblocked until the event occurs.</p>
<h4>Deadlocks</h4>
<p>A deadlock is a situation in which two or more processes are unable to process because each is waiting for one of the others to do something. A deadlock involves passive waiting, with a BLOCKED process state. The livelock alternative involves active waiting and an arbitrary process state. Deadlocks are the "lesser evil".</p>
<p>Necessary conditions for a deadlock:</p>
<ol>
<li>Exclusive allocation of resources ("mutual exclusion")</li>
<li>Allocation of additional resources ("hold and wait")</li>
<li>No removing of resources ("no preemption")</li>
<li>A closed chain of processes exists, such that each process holds at least one resource needed by the next process in the chain ("circular wait")</li>
</ol>
<p>Resources are administered by the operating system and provided to the processes. Resource allocation graphs are used to visualize and also automatically detect deadlock situations. They describe the current system state; The nodes are processes and resources, the edges show an allocation or a request.</p>
<p>Reusable resources are allocated by processes for a certain time and released again afterwards (CPU, main and mass storage, I/O devices, system data structures such as files, process table entries, etc.). A deadlock occurs if two processes each have allocated a reusable resource which is afterwards additionally requested by the respective other process. Access is typically synchronized with mutual exclusion.</p>
<p>Consumable resources are generated (produced) and destroyed (consumed) while the system is running (Interrupt requests, signals, messages, data from input devices, etc.). A deadlock occurs if two processes each wait for a consumable resource which is produced by the respective other process. Access is typically synchronized with one-sided synchronization.</p>
<p>Indirect methods for preventing deadlocks are 1) use non blocking approaches 2) only allow atomic resource allocations 3) enable the preemption of resources using virtualization. Direct methods prevent circular waiting with continuous requirements analysis and avoidance of "unsafe states".</p>
<p>Banker's algorithm is a deadlock avoidance algorithm which finds a process sequence that guarantees that the system does not run out of resources even when all processes completely use their "credit limit".</p>
<p>Deadlocks can be accepted ("ostrich algorithm") or detected by creating a waiting graph and search for cycles (O(n)). In the recovery phase, deadlocked processes are terminated and resources are preempted. Methods to avoid/detect deadlocks are very difficult to implement, require too much overhead and are thus not useable. Prevention methods more commonly used and relevant in practice. The risk of deadlock can also be solved by virtualizing resources.</p>
<h1>Questions</h1>
<h2>An Introduction to Operating Systems</h2>
<ul>
<li>Why were operating systems developed initially?</li>
<li>How did the features of operating systems evolve along with the development of the available hardware?</li>
<li>How do we define the term "operating system" today?</li>
<li>What are the building blocks of a computer system?
<ul>
<li>CPU, RAM, character and block I/O device, bus</li>
</ul>
</li>
<li>Which resources are represented by these building blocks?
<ul>
<li>Disk is a block device</li>
<li>Terminal and printer are character devices</li>
</ul>
</li>
<li>How does code (in the OS) interact with hardware resources?
<ul>
<li>Device handling, device drivers</li>
<li>Port and memory mapped I/O</li>
<li>Interrupts (async notification)</li>
<li>DMA</li>
</ul>
</li>
<li>What are the most relevant developments in computer architecture of the last decades and which problems/benefits are related to these developments?</li>
</ul>
<h2>Challenges and Tasks of Operating Systems</h2>
<ul>
<li>Which abstractions does a modern OS provide?
<ul>
<li>CPUs, processes, memory, file systems, security, ...</li>
</ul>
</li>
<li>What is a process?
<ul>
<li>How do processes interact with each other and the OS?
<ul>
<li>System calls</li>
<li>Synchronization and deadlock fundamentals</li>
</ul>
</li>
<li>What different view of processes exists and why?</li>
<li>Which states can a process have and what characterizes the different states?
<ul>
<li>What transitions between states are legal?</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2>Processes and Threads</h2>
<ul>
<li>What is the definition of a process and what is the difference to a program?</li>
<li>What is a process hierarchy and why does it exist?
<ul>
<li>Parent/child processes, orphans, zombies and PID 1 (init)</li>
</ul>
</li>
<li>How can processes perform I/O, how can it be (re)configured?
<ul>
<li>Relation of the I/O concept to the Unix philosophy?
<ul>
<li>Do one thing, and do it well</li>
<li>Everything is a file</li>
</ul>
</li>
</ul>
</li>
<li>How do processes interact with the OS: system calls
<ul>
<li>"Gate to the kernel"</li>
</ul>
</li>
<li>How can processes be created/controlled/terminated?
<ul>
<li>Which Unix syscalls are used for process management?
<ul>
<li>Fork, exec, kill etc.</li>
</ul>
</li>
<li>Pros and cons of the Unix fork/exec model</li>
</ul>
</li>
<li>Optimizations for process creation in Unix: copy-on-write</li>
<li>What are details of the extended process state model?</li>
<li>What is the overhead of Unix processes and their creation?</li>
<li>What are the differences between address spaces for processes and threads?</li>
<li>What are the thread models in Unix and Windows?</li>
<li>What are fibers (user-level threads)?
<ul>
<li>Can you discuss pros and cons of threads vs. fibers?</li>
</ul>
</li>
<li>Cooperative multithreading
<ul>
<li>Can you describe the ideas behind Duff’s device and protothreads? (you don’t have to know the details of their implementations)</li>
<li>Difference between the cooperative and preemptive ways</li>
</ul>
</li>
</ul>
<h2>Concurrency: Mutual exclusion, Synchronisation, Deadlocks</h2>
<ul>
<li>What is shared data/memory communication, why is it problematic?
<ul>
<li>Can you give an example of a problematic situation?
<ul>
<li>i.e. a race condition</li>
</ul>
</li>
<li>Can you understand multithreaded code using shared data?</li>
</ul>
</li>
<li>What is a race condition (can you give examples)?
<ul>
<li>Why are race conditions hard to detect and debug?</li>
</ul>
</li>
<li>What is synchronization used for, which options for synchronization exist?
<ul>
<li>Can you define the term "critical section"?</li>
</ul>
</li>
<li>What are locks and how are they used?
<ul>
<li>Can you give details on lock implementations (atomic operations, suppressing interrupts, semaphores)?</li>
</ul>
</li>
<li>What is a semaphore, which operations exists on semaphores?
<ul>
<li>Can you define the use and implementation of semaphores?</li>
<li>Can you describe problems (e.g. reader/writer) solved using semaphores?</li>
</ul>
</li>
<li>What are monitors and how to they differ from semaphore solutions?</li>
<li>Can you define the terms "deadlock" and "livelock"?
<ul>
<li>Explain situations leading to both problems</li>
</ul>
</li>
<li>What are the necessary conditions for deadlocks to occur?
<ul>
<li>What is the additional condition that is required for a deadlock to occur?</li>
</ul>
</li>
<li>Which types of resources exists related to synchronization?</li>
<li>What are the components of a resource allocation graph, how do you construct it?
<ul>
<li>How can you detect a deadlock in this graph?
<ul>
<li>Why is this not effective in practise?</li>
</ul>
</li>
</ul>
</li>
<li>What is the dining philosophers problem?
<ul>
<li>Why do deadlocks occur here?</li>
<li>Can you describe a solution to solve the problem?</li>
<li>Can you discuss the efficiency of different solutions?</li>
</ul>
</li>
<li>How can deadlocks be prevented and what are safe/unsafe states?</li>
<li>Which methods exist to resolve a deadlock and what are their pros/cons?</li>
</ul>
<h2>Memory management and Virtual Memory</h2>
<ul>
<li>Requirements for memory management for multiprogramming systems?</li>
<li>Which policies and strategies are relevant for memory management?</li>
<li>Can you describe the basic problem of memory allocation?</li>
<li>How does dynamic memory allocation work?
<ul>
<li>Can you describe different approaches, describe pros/cons?</li>
</ul>
</li>
<li>Can you name and describe different placement strategies?</li>
<li>What is memory fragmentation?
<ul>
<li>Which kinds of fragmentation exist, what are their properties?
<ul>
<li>Internal and external fragmentation</li>
</ul>
</li>
<li>Where are different allocation methods typically used?</li>
</ul>
</li>
<li>Can you describe differences between swapping, segmentation, paging?
<ul>
<li>How does paging as an OS concept interact with the MMU (and TLB, and page tables)?</li>
<li>How can paging be optimized using hardware or software approaches (like TLB)?</li>
</ul>
</li>
<li>What is the locality principle in computers?
<ul>
<li>Which kinds of locality exist, can you describe their properties?</li>
<li>How can locality be used to optimize performance?</li>
</ul>
</li>
<li>What is the idea behind virtual memory, which abstraction/illusion is created by virtual memory?</li>
<li>How does demand paging work?
<ul>
<li>What is a page fault and how is it handled?</li>
<li>What are tasks of the OS and hardware when handling page faults?</li>
</ul>
</li>
<li>Can you name different page replacement strategies and discuss their pros/cons? (FIFO, optimal, LRU, second chance)
<ul>
<li>Can you simulate different strategies given an access sequence?</li>
</ul>
</li>
<li>Can you define thrashing and name causes and possible solutions?</li>
<li>What is the working set of a process and how can you determine it?</li>
</ul>
<h2>Scheduling: Uni- and Multiprocessor</h2>
<ul>
<li>Which approaches to inter-process communication (IPC) exist?
<ul>
<li>Can you give their pros/cons?</li>
</ul>
</li>
<li>What are the primitives for message-based communication?
<ul>
<li>Which synchronization methods exist here?</li>
<li>How can processes be addressed?</li>
<li>Which message formats exist?</li>
</ul>
</li>
<li>Which IPC methods exist in Unix?</li>
<li>Can you describe the concepts and use (programming) of…
<ul>
<li>Signals, unnamed pipes, named pipes, Unix message queues, sockets</li>
</ul>
</li>
<li>What is RPC and what is the fundamental difference to IPC?</li>
<li>Can you define the terms "dispatching" and "scheduling"?</li>
<li>Which dispatch states exist, which level of scheduling are they related to?
<ul>
<li>Can you describe details of short/medium/long term scheduling?</li>
<li>Which process state transitions are related to which scheduling level?</li>
</ul>
</li>
<li>Can you explain preemptive scheduling and its advantages?
<ul>
<li>Can you give examples for scheduling strategies, explain how they work?</li>
<li>Can you determine scheduling orders for a given strategy?</li>
</ul>
</li>
<li>Can you discuss pros/cons of the different scheduling strategies?</li>
<li>What is multi-level scheduling and how is this related to priorities?</li>
<li>Can you give details of scheduling strategies in Unix and Windows?</li>
</ul>
<h2>I/O Management and Disk Scheduling</h2>
<ul>
<li>How do devices and the OS interact? Can you name different methods?</li>
<li>Which classes of devices exist and what are their properties?
<ul>
<li>Character devices, block devices etc.</li>
</ul>
</li>
<li>How do interrupts and DMA work and what are their pros/cons?</li>
<li>How can I/O devices be addressed by the OS?</li>
<li>What is a device driver, in which ways can it interact with the hardware?</li>
<li>What are the various tasks of the OS related to devices?</li>
<li>How are devices represented and abstracted in Unix?
<ul>
<li>Name properties of/differences between character/block/other devices</li>
<li>How does the OS implement the relation device special file &lt;=&gt; device driver?</li>
</ul>
</li>
<li>How can devices be used in user processes, what are related syscalls/libc functions?
<ul>
<li>Why is buffering important, can you discuss the pros/cons?</li>
<li>How does a ring buffer work, where is it typically used?</li>
</ul>
</li>
<li>How does I/O scheduling for disk drives work
<ul>
<li>What are the pros/cons of the different scheduling approaches?</li>
</ul>
</li>
</ul>
<h2>File Management</h2>
<ul>
<li>What is the file abstraction and why is it useful?</li>
<li>What are the syscalls/libc functions in Unix to handle files?</li>
<li>What is a virtual file system and how does this work?
<ul>
<li>What is mounting/unmounting, what is their effect on the directory tree of a Unix system?</li>
</ul>
</li>
<li>Which methods exist to map a file to disk blocks?
<ul>
<li>Describe problems of the approaches/pros/cons</li>
</ul>
</li>
<li>Which methods exist to manage free space?</li>
<li>What are the directory and inode structures for typical file systems?
<ul>
<li>Unix System V, BSD FFS, Linux ext2/3/4</li>
</ul>
</li>
<li>What are the challenges for file systems today?</li>
<li>How can the reliability of disk storage be improved?</li>
<li>How can the performance of disk storage be improved?</li>
<li>What is the Unix block buffer cache and how does it work?</li>
<li>What is logical volume management and why is it useful?</li>
<li>What is RAID?
<ul>
<li>Which different RAID levels exist and how do they work?</li>
<li>Can you discuss the pros/cons of the different levels?</li>
</ul>
</li>
<li>What is a journaling/log structured file system?
<ul>
<li>How do they work, what are differences to traditional FS?</li>
</ul>
</li>
</ul>
<h2>Virtual Machines and Microkernels</h2>
<ul>
<li>Can you define monolithic kernels, microkernels, hypervisors?
<ul>
<li>Differences between these, pros/cons?</li>
</ul>
</li>
<li>What problem did first-generation microkernels have?
<ul>
<li>How was this solved in second-generation microkernels?</li>
<li>What is an exokernel?</li>
</ul>
</li>
<li>What is virtualization, can you define its functionality?
<ul>
<li>What is a virtual machine monitor or hypervisor?</li>
<li>What are the differences between type 1 and 2 hypervisors?</li>
<li>Which hardware support was introduced to support virtualization?</li>
<li>What is paravirtualization and what are its pros/cons?</li>
<li>What is a hypercall?</li>
</ul>
</li>
</ul>
<h2>The Cloud, Unikernels and Single-Address Space OS's</h2>
<ul>
<li>Which service models exist for Cloud systems?
<ul>
<li>What are their properties, pros and cons?</li>
<li>Which provisioning models exist?</li>
</ul>
</li>
<li>What does the architecture for a Cloud OS look like?
<ul>
<li>What are differences to a "regular" OS?</li>
<li>Which strategic decisions have to be taken by a Cloud OS?</li>
</ul>
</li>
<li>What is a container and how are containers related to virtualization?
<ul>
<li>What is virtualized in containers?</li>
</ul>
</li>
<li>How does virtual memory management interact with virtualization for the Cloud?
<ul>
<li>Which optimization approaches exist, can you describe them?</li>
</ul>
</li>
<li>How can I/O be virtualized for the Cloud?
<ul>
<li>Which I/O virtualization approaches exist, can you name pros/cons?</li>
</ul>
</li>
</ul>
<h2>Operating System Security</h2>
<ul>
<li>Can you define safety and security?</li>
<li>What is the task of OS security?</li>
<li>Can you give examples for malware?</li>
<li>What is the difference to social engineering?</li>
<li>Which types of malware exist, can you define them?</li>
<li>What is permission management and what are the related requirements?</li>
<li>What is the principle of least privilege?</li>
<li>Define the access matrix and describe the ways to use it</li>
<li>File/process attributes in Unix</li>
<li>ACLs, capabilities, mandatory access control</li>
<li>How do the MMU and the CPU privilege levels contribute to security?</li>
<li>What is software-based protection, can you give an example?</li>
<li>Which typical software bugs contribute to security problems, can you give examples?
<ul>
<li>Buffer overflow</li>
</ul>
</li>
</ul>


</article>
</body>

</html>
