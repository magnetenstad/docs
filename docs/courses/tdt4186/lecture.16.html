<!DOCTYPE html>
<html>
  <head>
    <!-- Katex -->
    <link rel="stylesheet" href=
        "https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css">

    <!-- GitHub Markdown Styles -->
    <link rel="stylesheet" href=
        "https://cdn.jsdelivr.net/github-markdown-css/2.2.1/github-markdown.css"/>

    <title>lecture.16.md</title>
    <link rel="icon" type="image/x-icon" href="../../null">

    <!-- Custom Styles -->
    <link rel="stylesheet" href="../../styles.css">
  
  </head>

  <body class="markdown-body">
    <div class="page flex-row">
      <div class="links">
        <p></p>
<h4><a href="../index.html">courses/</a><a href="./index.html">tdt4186</a></h4>
<ul>
<li>📂 <a href="./assets/index.html">assets</a></li>
<li>📄 <a href="compendium.html">compendium.html</a></li>
<li>📄 <a href="lecture.1.html">lecture.1.html</a></li>
<li>📄 <a href="lecture.10.html">lecture.10.html</a></li>
<li>📄 <a href="lecture.11.html">lecture.11.html</a></li>
<li>📄 <a href="lecture.12.html">lecture.12.html</a></li>
<li>📄 <a href="lecture.13.html">lecture.13.html</a></li>
<li>📄 <a href="lecture.14.html">lecture.14.html</a></li>
<li>📄 <a href="lecture.15.html">lecture.15.html</a></li>
<li>📄 <a href="lecture.16.html">lecture.16.html ✨</a></li>
<li>📄 <a href="lecture.17.html">lecture.17.html</a></li>
<li>📄 <a href="lecture.18.html">lecture.18.html</a></li>
<li>📄 <a href="lecture.19.html">lecture.19.html</a></li>
<li>📄 <a href="lecture.2.html">lecture.2.html</a></li>
<li>📄 <a href="lecture.20.html">lecture.20.html</a></li>
<li>📄 <a href="lecture.21.html">lecture.21.html</a></li>
<li>📄 <a href="lecture.22.html">lecture.22.html</a></li>
<li>📄 <a href="lecture.3.html">lecture.3.html</a></li>
<li>📄 <a href="lecture.4.html">lecture.4.html</a></li>
<li>📄 <a href="lecture.5.html">lecture.5.html</a></li>
<li>📄 <a href="lecture.6.html">lecture.6.html</a></li>
<li>📄 <a href="lecture.7.html">lecture.7.html</a></li>
<li>📄 <a href="lecture.8.html">lecture.8.html</a></li>
<li>📄 <a href="lecture.9.html">lecture.9.html</a></li>
<li>📄 <a href="questions.html">questions.html</a>
</li>
</ul>

      </div>
      <article class="content">
        <h1>lecture.16.html</h1>
<h2>Lecture 16: Modern file systems</h2>
<p><a href="lecture.15.html">Previous lecture</a>
<a href="lecture.17.html">Next lecture</a></p>
<p><iframe
width="560" height="315" src="https://www.youtube.com/embed/lfHEBzFOhqM"
title="YouTube video player"
frameborder="0"
allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
allowfullscreen>
</iframe></p>
<h3>Exam</h3>
<p>Modern file systems (FS)</p>
<p><strong>Important questions:</strong></p>
<ul>
<li>What are the challenges for file systems today?</li>
<li>How can the reliability of disk storage be improved?</li>
<li>How can the performance of disk storage be improved?</li>
<li>What is the Unix block buffer cache and how does it work?</li>
<li>What is logical volume management and why is it useful?</li>
<li>What is RAID?
<ul>
<li>Which different RAID levels exist and how do they work?</li>
<li>Can you discuss the pros/cons of the different levels?</li>
</ul>
</li>
<li>What is a journaling/log structured file system?
<ul>
<li>How do they work, what are differences to traditional FS?</li>
</ul>
</li>
</ul>
<h3>What do we know about storage so far?</h3>
<ul>
<li>Disk drives have a block structure and provide random access
<ul>
<li>Locality of disk accesses is crucial to enable high performance
<ul>
<li>head movements are especially costly – multiple ms</li>
</ul>
</li>
<li>SSDs are also block structured but have no mechanical delays</li>
</ul>
</li>
<li>File systems provide abstractions to enable programs to work with persistent data
<ul>
<li>Files and directory hierarchies</li>
<li>Metadata, e.g. name, size, file creation data, …</li>
</ul>
</li>
<li>There are different ways to map file systems onto a disk</li>
</ul>
<h3>Challenge: Reliability (1)</h3>
<ul>
<li>Problems:
<ul>
<li>Defective disks or blocks</li>
<li>System crashes or failures</li>
</ul>
</li>
<li>Impacts:
<ul>
<li>Complete loss of data</li>
<li>Defective data blocks, e.g.
<ul>
<li>application can no longer read a file</li>
</ul>
</li>
<li>Inconsistent metadata, e.g.
<ul>
<li>directory entry for a file is missing or vice versa</li>
<li>block is used but marked as free</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img src="assets/lecture.16.bath_tub_curve.png" alt="">
The "bath tub curve" shows the typical development of the error rate of hard disks (and most other technical products) over their lifetime</p>
<h3>Challenge: Reliability (2)</h3>
<ul>
<li>Solution: Backup
<ul>
<li>Frequent incremental and complete backup of data to a different storage medium</li>
<li>Problems: time and storage space overhead</li>
</ul>
</li>
<li>Solution: Checksums
<ul>
<li>Files can be annotated with a checkum (error detection) or an error-correcting code (repair)</li>
<li>Problems: storage space overhead; responsibility (layer)</li>
</ul>
</li>
<li>Solution: Repair tools
<ul>
<li>Programs such as chkdsk, scandisk or fsck are able to repair (some) inconsistent metadata</li>
<li>Problems: Possible loss of data in the repair process; long runtimes of the repair programs for large disks</li>
</ul>
</li>
</ul>
<h3>Challenge: Performance optimization</h3>
<ul>
<li>Problem:
<ul>
<li>Hard disks have low read/write speeds and a high positioning latency</li>
<li>CPU/main memory performance and disk performance diverge</li>
<li>Impact:</li>
<li>The hard disk becomes the bottleneck for I/O intensive applications (e.g. databases) and tasks (e.g. booting the system or starting a program)</li>
</ul>
</li>
<li>Solution: Cache
<ul>
<li>Keep important (meta)data in main memory</li>
<li>Problem: Consistency between cache and disk</li>
</ul>
</li>
</ul>
<h3>Challenge: Disk management</h3>
<ul>
<li>Problem:
<ul>
<li>Physical dimensions of disk drives limit the size of the file system(s)</li>
<li>What can be done if a disk is full? Impact:</li>
<li>Disk capacity is over dimensioned to avoid the overhead of copying to a different disk</li>
</ul>
</li>
<li>Solution: Virtual file system
<ul>
<li>Mount new disks as directories (using "soft links")</li>
<li>Problems: not transparent for users and applications; size limitation still in place for existing directories</li>
</ul>
</li>
</ul>
<p><img src="assets/lecture.16.disk_management.png" alt=""></p>
<h3>Intelligent block device (drivers)</h3>
<ul>
<li>Idea: Handle reliability problems below the file system layer</li>
<li>Advantage: all file system implementations can benefit</li>
</ul>
<p><img src="assets/lecture.16.intelligent_block_device.png" alt=""></p>
<h3>UNIX Block Buffer Cache</h3>
<ul>
<li>Buffer for disk blocks in main memory
<ul>
<li>Uses algorithms similar to page frame handling</li>
<li>Read ahead: for sequential reads, the transfer of subsequent data blocks is initiated</li>
<li>Lazy write: a block is not written to disk directly</li>
<li>allows optimization of write accesses and does not block the writer</li>
<li>Free block management in a free list
<ul>
<li>Possible entries for the free list are determined using LRU</li>
<li>Blocks which are already marked free but are not yet reused can be reactivated (reclaim)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3>UNIX Block Buffer Cache (2)</h3>
<ul>
<li>Write to disk if/when
<ul>
<li>no more free buffers are available</li>
<li>periodically by the system (fsflush process, update process),</li>
<li>when calling the sync(2) system call</li>
<li>and after each write system call when the corresponding file was opened with the option O_SYNC</li>
</ul>
</li>
<li>Adressing
<ul>
<li>Blocks are addressed using a tuple:
<ul>
<li>(device number, block number)</li>
</ul>
</li>
<li>A hash of the address is used to select one of the possible buffer lists</li>
</ul>
</li>
</ul>
<h3>UNIX Block Buffer Cache: Structure</h3>
<p><img src="assets/lecture.16.unix_block_buffer_cache_structure.png" alt=""></p>
<h3>UNIX Block Buffer Cache: Structure (2)</h3>
<p><img src="assets/lecture.16.unix_block_buffer_cache_structure_2.png" alt=""></p>
<h3>[Linux] Logical Volume Management</h3>
<ul>
<li>1:1 relation between file system and disk is no longer enforced</li>
</ul>
<p><img src="assets/lecture.16.linux_logical_volume_management.png" alt=""></p>
<h3>Redundant Arrays of Inexpensive Disks</h3>
<p>(short: RAID)</p>
<ul>
<li>Initial idea: save costs by creating large logical disks out of inexpensive smaller disks (cost per GB)</li>
<li>Additional features:
<ul>
<li>better utilization of the available data bandwidth by using parallel transfers</li>
<li>fault tolerance using redundancy</li>
</ul>
</li>
<li>Two variants:
<ul>
<li>Hardware RAID: disk controller with special management software (+potentially cache)</li>
<li>Software RAID: layer between disk driver and file system code</li>
</ul>
</li>
</ul>
<h3>RAID 0: Disk striping</h3>
<ul>
<li>Idea: Data of a large logical disk are stored in a round robin way distributed over N physical disks:</li>
<li>Effect: increased bandwidth, since multiple disks are accessed in parallel</li>
<li>Disadvantage: failure probability is multiplied by N</li>
</ul>
<p><img src="assets/lecture.16.raid_0.png" alt=""></p>
<h3>RAID 1: Disk mirroring</h3>
<ul>
<li>Idea: data is stored redundantly on two disks at the same time:</li>
<li>Effect: increased read bandwidth, somewhat lower write bandwidth, higher reliability by having a copy of the data</li>
<li>Disadvantage: uses twice the disk space</li>
</ul>
<p><img src="assets/lecture.16.raid_1.png" alt=""></p>
<h3>RAID 4: Additional parity disk</h3>
<ul>
<li>Idea: data is striped over multiple disks, one disk stores the related parity</li>
<li>Effect: errors (of a single disk) can be detected and fixed without a large storage overhead. Fast read operations</li>
<li>Disadvantage: parity disk is bottleneck when writing</li>
</ul>
<p><img src="assets/lecture.16.raid_4.png" alt=""></p>
<h3>RAID 5 and 6: Distributed parity data</h3>
<ul>
<li>Idea: distribute the parity block over all disks</li>
<li>Effect: additional write overhead to update the parity block when writing is distributed
<ul>
<li>With RAID 6, an additional parity block can be used to restore the data of two failed disks</li>
</ul>
</li>
<li>Disadvantage: all data is protected with high overhead, even though a part of the data may be not critical</li>
</ul>
<p><img src="assets/lecture.16.raid_5_6.png" alt=""></p>
<h3>RAID x+y (= RAID xy): Hierarchies</h3>
<ul>
<li>Idea: Combine different RAID mechanisms in a hierarchy, e.g. RAID 1+0 (= RAID 10):</li>
<li>Effect: properties can be combined. Common setups: RAID 10, 50 or 60</li>
<li>Disadvantage: requires a large number of disks</li>
</ul>
<p><img src="assets/lecture.16.raid_x_y.png" alt=""></p>
<h3>Journaled File Systems</h3>
<ul>
<li>In addition to writing data and metadata (e.g. inodes), journaled file systems write a protocol of the changes
<ul>
<li>All changes are part of a transaction</li>
<li>Examples for transactions:
<ul>
<li>create, delete, expand, shorten files</li>
<li>change file attributes</li>
<li>rename a file</li>
</ul>
</li>
<li>All changes to the file system are additionally stored in a protocol file (log file)</li>
<li>At boot time, the protocol file is compared to the latest changes, this avoids inconsistencies</li>
</ul>
</li>
</ul>
<h3>Journaled File Systems: Protocol</h3>
<ul>
<li>A protocol entry is generated for each single operation of a transaction and…
<ul>
<li>after this, the change to a file system is carried out</li>
</ul>
</li>
<li>Important conditions:
<ul>
<li>A protocol entry is always written to disk before the change itself</li>
<li>If something was changed on a disk, the related protocol entry is also found on that disk</li>
</ul>
</li>
</ul>
<h3>Journaled File Systems: Recovery</h3>
<ul>
<li>When booting a system, the operating system checks, if the changes logged in the protocol are present on disk:</li>
<li>A transaction can be repeated or committed if all protocol entries are available on disk → redo</li>
<li>Started transactions that have not been completed are revoked → undo</li>
</ul>
<h3>Journaled File Systems: Results</h3>
<ul>
<li>Advantages:
<ul>
<li>a transaction is either committed (completed) in whole or not at all</li>
<li>users can define transactions that span multiple file accesses, if these are also recorded in the log</li>
<li>impossible to create inconsistent metadata</li>
<li>booting a crashed system only requires a fast log file check
<ul>
<li>the alternative chkdsk takes a long time for large disks</li>
</ul>
</li>
</ul>
</li>
<li>Disadvantages:
<ul>
<li>less efficient, since additional log file has to be written</li>
<li>thus usually only metadata journaling, no full journaling</li>
<li>examples: Windows NTFS, Linux ext 3/4, IBM JFS</li>
</ul>
</li>
</ul>
<h3>Log-structured file systems [1]</h3>
<p>(short: LFS)</p>
<ul>
<li>Observation:
<ul>
<li>Large caches reduce the frequency of read operations</li>
<li>Write operations should not be scattered</li>
</ul>
</li>
<li>(Radical) approach: one log is sufficient for everything!
<ul>
<li>Blocks are not overwritten, but only appended to the log</li>
<li>Changes to metadata are also stored in the log only</li>
<li>Write operations are collected in main memory and then written to disk as a single large segment (e.g. 1 MB)</li>
<li>Only the superblock has a fixed position on the disk</li>
</ul>
</li>
</ul>
<h3>Log-structured file systems (2)</h3>
<ul>
<li>Example:</li>
</ul>
<p><img src="assets/lecture.16.lfs.png" alt=""></p>
<ul>
<li>Log works like a ring buffer: changes are added to the front, obsolete data fall out at the end</li>
<li>Cleaner: process to compactify/release segments</li>
<li>Effect:
<ul>
<li>Consistency: new segments are entirely visible or not at all</li>
<li>The disk bandwidth is also utilized to a high degree when writing</li>
<li>Performance reduced significantly if free memory is low</li>
</ul>
</li>
</ul>
<h3>CoW: Copy-on-Write file systems</h3>
<ul>
<li>Many modern file systems refrain from overwriting
<ul>
<li>Idea from LFS, but more flexible when allocation free areas</li>
</ul>
</li>
<li>Example: manipulate file (B+ tree)… [2]</li>
</ul>
<p><img src="assets/lecture.16.cow.png" alt=""></p>
<ul>
<li>Example: "copy" complete directory tree</li>
</ul>
<p><img src="assets/lecture.16.cow_2.png" alt=""></p>
<h3>BTRFS: “butter” FS [2]</h3>
<p>… according to developer Chris Mason ("comes from a CoW")</p>
<ul>
<li>Widely used on Linux, inspired by Sun ZFS</li>
<li>Features: …very many…
<ul>
<li>Fast writes: Special "CoW friendly" B+ trees</li>
<li>Resource-saving snapshots</li>
<li>No loss of data
<ul>
<li>Atomic changes and checksums for all metadata and data</li>
</ul>
</li>
<li>Use of multiple disks
<ul>
<li>Implements flexible RAID: differentiates between data and metadata</li>
</ul>
</li>
<li>Size changes while the system is running</li>
<li>Data compression</li>
</ul>
</li>
</ul>
<h3>Conclusion</h3>
<ul>
<li>Modern file systems…
<ul>
<li>consider the properties of current hardware: large main memories (cache), fast parallel CPU cores, …</li>
<li>have many new features: snapshots, volume management, redundancy, …</li>
</ul>
</li>
<li>Basic design decision: Should this functionality be implemented in the file system (or rather at a lower layer)?
<ul>
<li>Pro:
<ul>
<li>more flexibility</li>
<li>possible to make use of knowledge about the file system structure, e.g. different RAID levels for data and metadata</li>
</ul>
</li>
<li>Con:
<ul>
<li>All file systems would benefit from functionality implemented on the driver level</li>
</ul>
</li>
</ul>
</li>
</ul>

      </article>
      </div>
  </body>
</html>
